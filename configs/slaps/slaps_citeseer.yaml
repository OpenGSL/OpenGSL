model:
  nlayers_adj: 2
  hidden_adj: 1024
  k: 30
  knn_metric: 'cosine'
  i: 6
  non_linearity: 'relu'
  normalization: 'sym'
  mlp_h: 3703
  mlp_epochs: 100
  mlp_act: 'relu'
  hidden: 32
  nlayers: 2
  dropout1: 0.5
  dropout2: 0.5
  dropout_adj1: 0.25
  dropout_adj2: 0.5

dataset:
  feat_norm: false
  cora_split: true # for cora,citeseer and pubmed
  feat_type: 'binary' # 'continuous' for pubmed, amazon-ratings, questions, roman-empire and wiki-cooc, otherwise 'binary'

training:
  data_cpu: false # whether data should be placed in cpu instead of gpu to save space
  lr: 1e-2
  lr_dae: 1e-3
  n_epochs: 2000
  epoch_d: 5
  weight_decay: 5e-4
  weight_decay_dae: 0
  lamda: 10
  ratio: 10
  nr: 5

analysis:
  flag: false
  project: gsl
