model:
  n_layers: 2
  act: relu # [relu, elu, gelu, leakyrelu]
  n_hidden: 64
  dropout: 0
  input_dropout: 0
  norm_type: LayerNorm # [LayerNorm, BatchNorm1d]
  n_heads: 8
  ff: false
  hidden_dim_multiplier: 1

dataset:
  feat_norm: true
  cora_split: false # for cora,citeseer and pubmed
  load_graph: false

training:
  data_cpu: false # whether data should be placed in cpu instead of gpu to save space
  lr: 3e-5
  n_epochs: 10
  weight_decay: 0

# analysis
analysis:
  flag: true
  project: gsl
  save_graph: false
  graph_analysis: false