model:
  n_layers: 5
  n_linear: 2 # layers of linear per gcn layer
  act: gelu # [relu, elu, gelu, leakyrelu]
  n_hidden: 512
  dropout: 0.2
  input_dropout: 0.2
  norm:
    flag: true
    norm_type: LayerNorm # [LayerNorm, BatchNorm1d]
  input_layer: true
  output_layer: true
  spmm_type: 1 # specilized for sparse mltiply [0,1], expected to remove in future versions

gsl:
  model_type: dense # [diag, dense]
  n_layers: 2
  n_linear: 1 # layers of linear per gcn layer
  act: tanh # [relu, elu, gelu, leakyrelu]
  n_hidden: 128
  dropout: 0.2
  input_dropout: 0.2
  norm:
    flag: false
    norm_type: LayerNorm # [LayerNorm, BatchNorm1d]
  input_layer: false
  output_layer: false
  spmm_type: 1 # specilized for sparse mltiply [0,1], expected to remove in future versions
  K: 300
  normalize: true

dataset:
  re_split: false # for cora,citeseer and pubmed

training:
  data_cpu: false # whether data should be placed in cpu instead of gpu to save space
  lr: 3e-5
  n_epochs: 1500
  weight_decay: 0
  lr_graph: 3e-5

analysis:
  flag: true
  project: gsl
  group: group2

# use wandb to tune hyperparams
sweep:
  flag: false
  sweep_config:
    method: grid
    name: grcn
    description: cora
    metric:
      goal: maximize
      name: loss
    parameters:
      n_layers:
        values: [3, 4, 5]
      n_linear:
        values: [1, 2]
      act:
        values: [relu, elu]
      dropout:
        min: 0.2
        max: 0.8
      K:
        [100, 200, 300, 400]
      lr:
        min: 0.0001
        max: 0.1
      epochs:
        values: [5, 10, 15]
