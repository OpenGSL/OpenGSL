model:
  n_layers: 2
  n_linear: 1 # layers of linear per gcn layer
  act: relu # [relu, elu, gelu, leakyrelu]
  n_hidden: 64
  dropout: 0.5
  graph_skip_conn: 0.5
  graph_learn_num_pers: 1
  no_bn: false
  hops: 5
  alpha: 0.1
  mlp_layers: 2
  input_dropout: 0
  norm:
    flag: false
    norm_type: BatchNorm1d # [LayerNorm, BatchNorm1d]
  input_layer: false
  output_layer: false
  spmm_type: 0 # specilized for sparse mltiply [0,1], expected to remove in future versions

dataset:
  cora_split: false # for cora,citeseer and pubmed
  feat_norm: true
  normalize: true
  add_loop: true
  sparse: true
#  homophily_control: 0.8

training:
  data_cpu: false # whether data should be placed in cpu instead of gpu to save space
  lr: 1e-1
  n_epochs: 500
  weight_decay: 0
  patience: ~
  criterion: metric

# analysis
analysis:
  flag: false
  project: gsl
  save_graph: false
  load_graph: false
  load_graph_path: results/graph/gen

tune:
  flag: false
  study: wsgnn_blogcatalog
  n_trials: ~
  database: sqlite:///wsgnn_blogcatalog.db
  n_exp_per_trial: 1
  params:
    lr:
      type: categorical
      values: [1e-1,1e-2,1e-3,1e-4]
    weight_decay:
      type: categorical
      values: [5e-4,5e-5,0]
    n_hidden:
      type: categorical
      values: [64,128]
    hops:
      type: discrete_uniform
      values: [1,10,1]
    graph_skip_conn:
      type: discrete_uniform
      values: [0.5, 0.9, 9.1]
    graph_learn_num_pers:
      type: categorical
      values: [1,2,4,6,8]