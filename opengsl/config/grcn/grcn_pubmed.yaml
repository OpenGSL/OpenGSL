model:
  type: gcn
  n_layers: 2
  n_linear: 1 # layers of linear per gcn layer
  act: F.relu # [relu, elu, gelu, leakyrelu]
  n_hidden: 32
  dropout: 0.5
  input_dropout: 0
  norm: ~
  input_layer: false
  output_layer: false
  spmm_type: 1 # specilized for sparse mltiply [0,1], expected to remove in future versions

gsl:
  model_type: dense
  K: 1
  normalize: false
  n_layers: 2
  n_linear: 1 # layers of linear per gcn layer
  act: tanh # [relu, elu, gelu, leakyrelu]
  n_hidden_1: 100
  n_hidden_2: 10
  dropout: 0
  input_dropout: 0
  norm: ~
  input_layer: false
  output_layer: false
  spmm_type: 1 # specilized for sparse mltiply [0,1], expected to remove in future versions

dataset:
  feat_norm: false   # grcn does not normalize feats
  add_loop: true
  sparse: true

training:
  data_cpu: false # whether data should be placed in cpu instead of gpu to save space
  lr: 5e-3
  n_epochs: 300
  weight_decay: 5e-3
  lr_graph: 0
  patience: ~
  criterion: ~

analysis:
  flag: false
  save_graph: false
  save_graph_path: results/graph
