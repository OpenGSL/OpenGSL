model:
  n_layers: 5
  act: F.relu # [relu, elu, gelu, leakyrelu]
  n_hidden: 128
  dropout: 0
  input_dropout: 0
  norm_type: LayerNorm # [LayerNorm, BatchNorm1d]
  n_heads: 8
  ff: false
  hidden_dim_multiplier: 1
  use_norm: false
  use_residual: false
  input_layer: true
  output_layer: false

dataset:
  feat_norm: false
  cora_split: false
  load_graph: false
  sparse: true

training:
  data_cpu: false # whether data should be placed in cpu instead of gpu to save space
  lr: 1e-3
  n_epochs: 1000
  weight_decay: 5e-7
  patience: ~
  criterion: ~

# analysis
analysis:
  flag: false
  save_graph: false
  save_graph_path: results/graph