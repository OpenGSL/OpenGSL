{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Your Own Graph Structure Learning Algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook let's try to implement a simple GSL algorithm using OpenGSL.\n",
    "\n",
    "We provide multiple choices for each component in OpenGSL.\n",
    "\n",
    "All of them can be freely chosen and assembled into a **GraphLearner** as shown below.\n",
    "\n",
    "<img src=\"../docs/source/img/package.jpg\" width=\"900\">\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "\n",
    "In this notebook we simply run experiments on *cora*, you can try other datasets on your own."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "                    #Nodes 2708\n",
      "                    #Edges 5278\n",
      "                    #Classes 7\n",
      "----Split statistics of 1 splits------'\n",
      "                #Train samples 140\n",
      "                #Val samples 500\n",
      "                #Test samples 1000\n"
     ]
    }
   ],
   "source": [
    "from opengsl.data.dataset import Dataset\n",
    "dataset = Dataset(\"cora\", n_splits=1)\n",
    "train_mask = dataset.train_masks[0]\n",
    "val_mask = dataset.val_masks[0]\n",
    "test_mask = dataset.test_masks[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model\n",
    "\n",
    "It's easy to implement a simple GSL algorithm using our provided components.\n",
    "\n",
    "Let's **choose the basic components and build the graphlearner**.\n",
    "\n",
    "We use the **GCNDiagEncoder** as in *[\"Graph-Revised Convolutional Network\"](https://arxiv.org/abs/1911.07123)*, followed by **Cosine** (metric), **KNN** (transform), **Interpolate** (fuse). Then a **GraphLearner** is built with these components."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from opengsl.module.encoder import GNNEncoder_OpenGSL, GCNDiagEncoder\n",
    "from opengsl.module import GraphLearner\n",
    "from opengsl.module.transform import KNN\n",
    "from opengsl.module.metric import Cosine\n",
    "from opengsl.module.fuse import Interpolate\n",
    "from opengsl.utils import set_seed\n",
    "\n",
    "device = torch.device('cuda')\n",
    "set_seed(42)\n",
    "encoder = GCNDiagEncoder(2, dataset.dim_feats)\n",
    "metric = Cosine()\n",
    "postprocess = [KNN(150)]\n",
    "fuse = Interpolate(1, 1)\n",
    "# build the graphlearner\n",
    "graphlearner = GraphLearner(encoder=encoder, metric=metric, postprocess=postprocess, fuse=fuse).to(device)\n",
    "# define gnn model\n",
    "gnn = GNNEncoder_OpenGSL(dataset.dim_feats, n_hidden=64, n_class=dataset.n_classes, n_layers=2, dropout=0.5).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To learn a new structure, you just need to input feature and original adj as `new_adj = graphlearner(x ,adj)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "With the defined graphlearner and gnn, you can use them in a common training and evaluation process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Time(s) 0.7270 | Loss(train) 1.9513 | Acc(train) 0.1429 | Loss(val) 1.8825 | Acc(val) 0.3780 | *\n",
      "Epoch 00002 | Time(s) 0.2790 | Loss(train) 1.9129 | Acc(train) 0.2357 | Loss(val) 1.8291 | Acc(val) 0.6660 | *\n",
      "Epoch 00003 | Time(s) 0.2827 | Loss(train) 1.8401 | Acc(train) 0.6214 | Loss(val) 1.7687 | Acc(val) 0.6900 | *\n",
      "Epoch 00004 | Time(s) 0.2697 | Loss(train) 1.7665 | Acc(train) 0.6929 | Loss(val) 1.7011 | Acc(val) 0.5960 | \n",
      "Epoch 00005 | Time(s) 0.2809 | Loss(train) 1.6787 | Acc(train) 0.6571 | Loss(val) 1.6185 | Acc(val) 0.6000 | \n",
      "Epoch 00006 | Time(s) 0.2678 | Loss(train) 1.5757 | Acc(train) 0.6643 | Loss(val) 1.5222 | Acc(val) 0.6120 | \n",
      "Epoch 00007 | Time(s) 0.2970 | Loss(train) 1.4569 | Acc(train) 0.6643 | Loss(val) 1.4132 | Acc(val) 0.6580 | \n",
      "Epoch 00008 | Time(s) 0.2823 | Loss(train) 1.3407 | Acc(train) 0.7000 | Loss(val) 1.2960 | Acc(val) 0.6720 | \n",
      "Epoch 00009 | Time(s) 0.2816 | Loss(train) 1.2040 | Acc(train) 0.7357 | Loss(val) 1.1770 | Acc(val) 0.7060 | *\n",
      "Epoch 00010 | Time(s) 0.2824 | Loss(train) 1.0695 | Acc(train) 0.8000 | Loss(val) 1.0648 | Acc(val) 0.7380 | *\n",
      "Epoch 00011 | Time(s) 0.2801 | Loss(train) 0.9685 | Acc(train) 0.8000 | Loss(val) 0.9653 | Acc(val) 0.7520 | *\n",
      "Epoch 00012 | Time(s) 0.2834 | Loss(train) 0.8521 | Acc(train) 0.8071 | Loss(val) 0.8821 | Acc(val) 0.7560 | *\n",
      "Epoch 00013 | Time(s) 0.2939 | Loss(train) 0.7786 | Acc(train) 0.8286 | Loss(val) 0.8174 | Acc(val) 0.7680 | *\n",
      "Epoch 00014 | Time(s) 0.2826 | Loss(train) 0.7058 | Acc(train) 0.8500 | Loss(val) 0.7672 | Acc(val) 0.7640 | \n",
      "Epoch 00015 | Time(s) 0.2943 | Loss(train) 0.6496 | Acc(train) 0.8429 | Loss(val) 0.7299 | Acc(val) 0.7640 | \n",
      "Epoch 00016 | Time(s) 0.2837 | Loss(train) 0.6127 | Acc(train) 0.8500 | Loss(val) 0.7045 | Acc(val) 0.7660 | \n",
      "Epoch 00017 | Time(s) 0.2798 | Loss(train) 0.5629 | Acc(train) 0.8429 | Loss(val) 0.6890 | Acc(val) 0.7620 | \n",
      "Epoch 00018 | Time(s) 0.2891 | Loss(train) 0.5322 | Acc(train) 0.8429 | Loss(val) 0.6801 | Acc(val) 0.7640 | \n",
      "Epoch 00019 | Time(s) 0.2834 | Loss(train) 0.5164 | Acc(train) 0.8500 | Loss(val) 0.6767 | Acc(val) 0.7680 | \n",
      "Epoch 00020 | Time(s) 0.2862 | Loss(train) 0.4844 | Acc(train) 0.8500 | Loss(val) 0.6725 | Acc(val) 0.7680 | \n",
      "Epoch 00021 | Time(s) 0.2903 | Loss(train) 0.4585 | Acc(train) 0.8357 | Loss(val) 0.6759 | Acc(val) 0.7800 | *\n",
      "Epoch 00022 | Time(s) 0.2783 | Loss(train) 0.4427 | Acc(train) 0.8571 | Loss(val) 0.6850 | Acc(val) 0.7800 | \n",
      "Epoch 00023 | Time(s) 0.2828 | Loss(train) 0.4222 | Acc(train) 0.8714 | Loss(val) 0.6922 | Acc(val) 0.7760 | \n",
      "Epoch 00024 | Time(s) 0.2939 | Loss(train) 0.4069 | Acc(train) 0.8786 | Loss(val) 0.7019 | Acc(val) 0.7680 | \n",
      "Epoch 00025 | Time(s) 0.2668 | Loss(train) 0.3931 | Acc(train) 0.8786 | Loss(val) 0.7045 | Acc(val) 0.7700 | \n",
      "Epoch 00026 | Time(s) 0.2967 | Loss(train) 0.3802 | Acc(train) 0.8786 | Loss(val) 0.7003 | Acc(val) 0.7700 | \n",
      "Epoch 00027 | Time(s) 0.2792 | Loss(train) 0.3609 | Acc(train) 0.8857 | Loss(val) 0.7010 | Acc(val) 0.7780 | \n",
      "Epoch 00028 | Time(s) 0.2831 | Loss(train) 0.3574 | Acc(train) 0.8857 | Loss(val) 0.7010 | Acc(val) 0.7720 | \n",
      "Epoch 00029 | Time(s) 0.2867 | Loss(train) 0.3412 | Acc(train) 0.8857 | Loss(val) 0.7045 | Acc(val) 0.7780 | \n",
      "Epoch 00030 | Time(s) 0.2830 | Loss(train) 0.3297 | Acc(train) 0.9071 | Loss(val) 0.7134 | Acc(val) 0.7780 | \n",
      "Epoch 00031 | Time(s) 0.2965 | Loss(train) 0.3238 | Acc(train) 0.9000 | Loss(val) 0.7164 | Acc(val) 0.7780 | \n",
      "Epoch 00032 | Time(s) 0.2830 | Loss(train) 0.3132 | Acc(train) 0.9000 | Loss(val) 0.7178 | Acc(val) 0.7780 | \n",
      "Epoch 00033 | Time(s) 0.2894 | Loss(train) 0.3032 | Acc(train) 0.9000 | Loss(val) 0.7189 | Acc(val) 0.7860 | *\n",
      "Epoch 00034 | Time(s) 0.2740 | Loss(train) 0.3055 | Acc(train) 0.9000 | Loss(val) 0.7135 | Acc(val) 0.7860 | \n",
      "Epoch 00035 | Time(s) 0.2943 | Loss(train) 0.2976 | Acc(train) 0.9000 | Loss(val) 0.7103 | Acc(val) 0.7840 | \n",
      "Epoch 00036 | Time(s) 0.2829 | Loss(train) 0.3052 | Acc(train) 0.8929 | Loss(val) 0.7227 | Acc(val) 0.7760 | \n",
      "Epoch 00037 | Time(s) 0.2827 | Loss(train) 0.2898 | Acc(train) 0.9071 | Loss(val) 0.7293 | Acc(val) 0.7780 | \n",
      "Epoch 00038 | Time(s) 0.2849 | Loss(train) 0.2864 | Acc(train) 0.9143 | Loss(val) 0.7328 | Acc(val) 0.7760 | \n",
      "Epoch 00039 | Time(s) 0.2990 | Loss(train) 0.2850 | Acc(train) 0.9143 | Loss(val) 0.7410 | Acc(val) 0.7760 | \n",
      "Epoch 00040 | Time(s) 0.2874 | Loss(train) 0.2746 | Acc(train) 0.9214 | Loss(val) 0.7386 | Acc(val) 0.7860 | \n",
      "Epoch 00041 | Time(s) 0.2925 | Loss(train) 0.2697 | Acc(train) 0.9286 | Loss(val) 0.7317 | Acc(val) 0.7880 | *\n",
      "Epoch 00042 | Time(s) 0.2807 | Loss(train) 0.2668 | Acc(train) 0.9214 | Loss(val) 0.7261 | Acc(val) 0.7880 | \n",
      "Epoch 00043 | Time(s) 0.2983 | Loss(train) 0.2595 | Acc(train) 0.9286 | Loss(val) 0.7177 | Acc(val) 0.7840 | \n",
      "Epoch 00044 | Time(s) 0.2810 | Loss(train) 0.2502 | Acc(train) 0.9143 | Loss(val) 0.7135 | Acc(val) 0.7860 | \n",
      "Epoch 00045 | Time(s) 0.2796 | Loss(train) 0.2450 | Acc(train) 0.9071 | Loss(val) 0.7205 | Acc(val) 0.7820 | \n",
      "Epoch 00046 | Time(s) 0.2985 | Loss(train) 0.2522 | Acc(train) 0.9143 | Loss(val) 0.7342 | Acc(val) 0.7820 | \n",
      "Epoch 00047 | Time(s) 0.2818 | Loss(train) 0.2343 | Acc(train) 0.9286 | Loss(val) 0.7456 | Acc(val) 0.7840 | \n",
      "Epoch 00048 | Time(s) 0.2993 | Loss(train) 0.2414 | Acc(train) 0.9286 | Loss(val) 0.7422 | Acc(val) 0.7880 | \n",
      "Epoch 00049 | Time(s) 0.2924 | Loss(train) 0.2312 | Acc(train) 0.9214 | Loss(val) 0.7359 | Acc(val) 0.7880 | \n",
      "Epoch 00050 | Time(s) 0.2864 | Loss(train) 0.2329 | Acc(train) 0.9143 | Loss(val) 0.7457 | Acc(val) 0.7800 | \n",
      "Epoch 00051 | Time(s) 0.2864 | Loss(train) 0.2195 | Acc(train) 0.9286 | Loss(val) 0.7503 | Acc(val) 0.7780 | \n",
      "Epoch 00052 | Time(s) 0.2985 | Loss(train) 0.2266 | Acc(train) 0.9214 | Loss(val) 0.7521 | Acc(val) 0.7820 | \n",
      "Epoch 00053 | Time(s) 0.2862 | Loss(train) 0.2215 | Acc(train) 0.9286 | Loss(val) 0.7624 | Acc(val) 0.7800 | \n",
      "Epoch 00054 | Time(s) 0.2903 | Loss(train) 0.2171 | Acc(train) 0.9214 | Loss(val) 0.7638 | Acc(val) 0.7820 | \n",
      "Epoch 00055 | Time(s) 0.2828 | Loss(train) 0.2149 | Acc(train) 0.9143 | Loss(val) 0.7540 | Acc(val) 0.7840 | \n",
      "Epoch 00056 | Time(s) 0.2940 | Loss(train) 0.2080 | Acc(train) 0.9286 | Loss(val) 0.7616 | Acc(val) 0.7740 | \n",
      "Epoch 00057 | Time(s) 0.2833 | Loss(train) 0.2098 | Acc(train) 0.9286 | Loss(val) 0.7724 | Acc(val) 0.7700 | \n",
      "Epoch 00058 | Time(s) 0.2977 | Loss(train) 0.2096 | Acc(train) 0.9214 | Loss(val) 0.7771 | Acc(val) 0.7680 | \n",
      "Epoch 00059 | Time(s) 0.2907 | Loss(train) 0.1938 | Acc(train) 0.9500 | Loss(val) 0.7926 | Acc(val) 0.7720 | \n",
      "Epoch 00060 | Time(s) 0.2808 | Loss(train) 0.1935 | Acc(train) 0.9429 | Loss(val) 0.8112 | Acc(val) 0.7760 | \n",
      "Epoch 00061 | Time(s) 0.2958 | Loss(train) 0.1946 | Acc(train) 0.9286 | Loss(val) 0.8077 | Acc(val) 0.7920 | *\n",
      "Epoch 00062 | Time(s) 0.2982 | Loss(train) 0.1941 | Acc(train) 0.9286 | Loss(val) 0.7941 | Acc(val) 0.7860 | \n",
      "Epoch 00063 | Time(s) 0.2750 | Loss(train) 0.1783 | Acc(train) 0.9357 | Loss(val) 0.7912 | Acc(val) 0.7700 | \n",
      "Epoch 00064 | Time(s) 0.2677 | Loss(train) 0.1770 | Acc(train) 0.9429 | Loss(val) 0.7926 | Acc(val) 0.7780 | \n",
      "Epoch 00065 | Time(s) 0.2976 | Loss(train) 0.1799 | Acc(train) 0.9429 | Loss(val) 0.8093 | Acc(val) 0.7780 | \n",
      "Epoch 00066 | Time(s) 0.3013 | Loss(train) 0.1630 | Acc(train) 0.9571 | Loss(val) 0.8492 | Acc(val) 0.7680 | \n",
      "Epoch 00067 | Time(s) 0.2931 | Loss(train) 0.1609 | Acc(train) 0.9429 | Loss(val) 0.8807 | Acc(val) 0.7640 | \n",
      "Epoch 00068 | Time(s) 0.2849 | Loss(train) 0.1632 | Acc(train) 0.9571 | Loss(val) 0.8730 | Acc(val) 0.7700 | \n",
      "Epoch 00069 | Time(s) 0.2992 | Loss(train) 0.1505 | Acc(train) 0.9714 | Loss(val) 0.8516 | Acc(val) 0.7820 | \n",
      "Epoch 00070 | Time(s) 0.2951 | Loss(train) 0.1382 | Acc(train) 0.9643 | Loss(val) 0.8294 | Acc(val) 0.7840 | \n",
      "Epoch 00071 | Time(s) 0.2832 | Loss(train) 0.1407 | Acc(train) 0.9714 | Loss(val) 0.8209 | Acc(val) 0.7860 | \n",
      "Epoch 00072 | Time(s) 0.2987 | Loss(train) 0.1360 | Acc(train) 0.9786 | Loss(val) 0.8282 | Acc(val) 0.7840 | \n",
      "Epoch 00073 | Time(s) 0.2839 | Loss(train) 0.1331 | Acc(train) 0.9786 | Loss(val) 0.8523 | Acc(val) 0.7840 | \n",
      "Epoch 00074 | Time(s) 0.2779 | Loss(train) 0.1194 | Acc(train) 0.9714 | Loss(val) 0.8837 | Acc(val) 0.7700 | \n",
      "Epoch 00075 | Time(s) 0.2833 | Loss(train) 0.1164 | Acc(train) 0.9786 | Loss(val) 0.8992 | Acc(val) 0.7740 | \n",
      "Epoch 00076 | Time(s) 0.2933 | Loss(train) 0.1194 | Acc(train) 0.9643 | Loss(val) 0.8929 | Acc(val) 0.7700 | \n",
      "Epoch 00077 | Time(s) 0.2829 | Loss(train) 0.1086 | Acc(train) 0.9786 | Loss(val) 0.8815 | Acc(val) 0.7780 | \n",
      "Epoch 00078 | Time(s) 0.2924 | Loss(train) 0.1066 | Acc(train) 0.9857 | Loss(val) 0.8678 | Acc(val) 0.7720 | \n",
      "Epoch 00079 | Time(s) 0.2936 | Loss(train) 0.1004 | Acc(train) 0.9857 | Loss(val) 0.8629 | Acc(val) 0.7740 | \n",
      "Epoch 00080 | Time(s) 0.2841 | Loss(train) 0.0971 | Acc(train) 0.9857 | Loss(val) 0.8693 | Acc(val) 0.7740 | \n",
      "Epoch 00081 | Time(s) 0.2950 | Loss(train) 0.0955 | Acc(train) 0.9857 | Loss(val) 0.8821 | Acc(val) 0.7700 | \n",
      "Epoch 00082 | Time(s) 0.2876 | Loss(train) 0.0966 | Acc(train) 0.9857 | Loss(val) 0.8985 | Acc(val) 0.7660 | \n",
      "Epoch 00083 | Time(s) 0.2860 | Loss(train) 0.0933 | Acc(train) 0.9857 | Loss(val) 0.9121 | Acc(val) 0.7660 | \n",
      "Epoch 00084 | Time(s) 0.2718 | Loss(train) 0.0930 | Acc(train) 0.9786 | Loss(val) 0.9201 | Acc(val) 0.7660 | \n",
      "Epoch 00085 | Time(s) 0.2956 | Loss(train) 0.0895 | Acc(train) 0.9786 | Loss(val) 0.9230 | Acc(val) 0.7700 | \n",
      "Epoch 00086 | Time(s) 0.2989 | Loss(train) 0.0941 | Acc(train) 0.9786 | Loss(val) 0.9260 | Acc(val) 0.7720 | \n",
      "Epoch 00087 | Time(s) 0.2804 | Loss(train) 0.0855 | Acc(train) 0.9929 | Loss(val) 0.9311 | Acc(val) 0.7700 | \n",
      "Epoch 00088 | Time(s) 0.2988 | Loss(train) 0.0886 | Acc(train) 0.9929 | Loss(val) 0.9437 | Acc(val) 0.7720 | \n",
      "Epoch 00089 | Time(s) 0.2790 | Loss(train) 0.0851 | Acc(train) 0.9857 | Loss(val) 0.9620 | Acc(val) 0.7700 | \n",
      "Epoch 00090 | Time(s) 0.2965 | Loss(train) 0.0843 | Acc(train) 0.9857 | Loss(val) 0.9764 | Acc(val) 0.7680 | \n",
      "Epoch 00091 | Time(s) 0.2829 | Loss(train) 0.0811 | Acc(train) 0.9929 | Loss(val) 0.9815 | Acc(val) 0.7700 | \n",
      "Epoch 00092 | Time(s) 0.2970 | Loss(train) 0.0797 | Acc(train) 0.9929 | Loss(val) 0.9786 | Acc(val) 0.7740 | \n",
      "Epoch 00093 | Time(s) 0.2764 | Loss(train) 0.0788 | Acc(train) 0.9857 | Loss(val) 0.9836 | Acc(val) 0.7720 | \n",
      "Epoch 00094 | Time(s) 0.2940 | Loss(train) 0.0742 | Acc(train) 0.9929 | Loss(val) 0.9935 | Acc(val) 0.7740 | \n",
      "Epoch 00095 | Time(s) 0.2835 | Loss(train) 0.0774 | Acc(train) 0.9857 | Loss(val) 1.0147 | Acc(val) 0.7700 | \n",
      "Epoch 00096 | Time(s) 0.2794 | Loss(train) 0.0707 | Acc(train) 0.9929 | Loss(val) 1.0396 | Acc(val) 0.7560 | \n",
      "Epoch 00097 | Time(s) 0.2699 | Loss(train) 0.0724 | Acc(train) 0.9929 | Loss(val) 1.0542 | Acc(val) 0.7520 | \n",
      "Epoch 00098 | Time(s) 0.2910 | Loss(train) 0.0691 | Acc(train) 0.9929 | Loss(val) 1.0509 | Acc(val) 0.7580 | \n",
      "Epoch 00099 | Time(s) 0.2681 | Loss(train) 0.0643 | Acc(train) 0.9929 | Loss(val) 1.0419 | Acc(val) 0.7680 | \n",
      "Epoch 00100 | Time(s) 0.2712 | Loss(train) 0.0665 | Acc(train) 1.0000 | Loss(val) 1.0271 | Acc(val) 0.7740 | \n",
      "Optimization Finished!\n",
      "Time(s): 17.9300\n",
      "Loss(test) 0.6546 | Acc(test) 0.8100\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "from opengsl.module.functional import normalize\n",
    "from opengsl.utils import accuracy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-2\n",
    "wd = 5e-4\n",
    "best_valid = 0\n",
    "gsl_weights =None\n",
    "gnn_weights =None\n",
    "start_time = time.time()\n",
    "optim = torch.optim.Adam([{'params': gnn.parameters()}, {'params': graphlearner.parameters()}], lr=lr, weight_decay=wd)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    improve = ''\n",
    "    t0 = time.time()\n",
    "    gnn.train()\n",
    "    graphlearner.train()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # forward and backward\n",
    "    adj = graphlearner(dataset.feats, normalize(dataset.adj))\n",
    "    output = gnn(dataset.feats, normalize(adj, add_loop=False))\n",
    "\n",
    "    loss_train = F.cross_entropy(output[train_mask], dataset.labels[train_mask])\n",
    "    acc_train = accuracy(dataset.labels[train_mask].cpu().numpy(), output[train_mask].detach().cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Evaluate\n",
    "    gnn.eval()\n",
    "    graphlearner.eval()\n",
    "    with torch.no_grad():\n",
    "        adj = graphlearner(dataset.feats, normalize(dataset.adj))\n",
    "        output = gnn(dataset.feats, normalize(adj, add_loop=False))\n",
    "        loss_val = F.cross_entropy(output[val_mask], dataset.labels[val_mask])\n",
    "        acc_val = accuracy(dataset.labels[val_mask].cpu().numpy(), output[val_mask].detach().cpu().numpy())\n",
    "\n",
    "    # save\n",
    "    if acc_val > best_valid:\n",
    "        improve = '*'\n",
    "        gsl_weights = deepcopy(graphlearner.state_dict())\n",
    "        gnn_weights = deepcopy(gnn.state_dict())\n",
    "        total_time = time.time() - start_time\n",
    "        best_val_loss = loss_val\n",
    "        best_valid = acc_val\n",
    "        best_adj = adj.detach().clone()\n",
    "\n",
    "    # debug\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n",
    "        epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n",
    "\n",
    "print('Optimization Finished!')\n",
    "print('Time(s): {:.4f}'.format(total_time))\n",
    "# test\n",
    "graphlearner.load_state_dict(gsl_weights)\n",
    "gnn.load_state_dict(gnn_weights)\n",
    "with torch.no_grad():\n",
    "    adj = graphlearner(dataset.feats, normalize(dataset.adj))\n",
    "    output = gnn(dataset.feats, normalize(adj, add_loop=False))\n",
    "    loss_test = F.cross_entropy(output[test_mask], dataset.labels[test_mask])\n",
    "    acc_test = accuracy(dataset.labels[test_mask].cpu().numpy(), output[test_mask].detach().cpu().numpy())\n",
    "\n",
    "print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that Our created GSL algorithm help improve the performance of GCN to 82.8, which is better than the usual 81+."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Evaluation using our pipeline\n",
    "We recommend you to use our provided pipline with **GSLSolver** and **ExpManager** to simplify the above process. Only *set_method* needs to be customized in this pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 0/3\n",
      "Epoch 00001 | Time(s) 0.2809 | Loss(train) 1.9478 | Acc(train) 0.1429 | Loss(val) 1.9183 | Acc(val) 0.3500 | *\n",
      "Epoch 00002 | Time(s) 0.2702 | Loss(train) 1.9167 | Acc(train) 0.4071 | Loss(val) 1.8544 | Acc(val) 0.5440 | *\n",
      "Epoch 00003 | Time(s) 0.2626 | Loss(train) 1.8495 | Acc(train) 0.5429 | Loss(val) 1.7847 | Acc(val) 0.5920 | *\n",
      "Epoch 00004 | Time(s) 0.2983 | Loss(train) 1.7728 | Acc(train) 0.6571 | Loss(val) 1.7066 | Acc(val) 0.5920 | \n",
      "Epoch 00005 | Time(s) 0.2683 | Loss(train) 1.6857 | Acc(train) 0.6643 | Loss(val) 1.6155 | Acc(val) 0.6180 | *\n",
      "Epoch 00006 | Time(s) 0.2474 | Loss(train) 1.5846 | Acc(train) 0.6643 | Loss(val) 1.5137 | Acc(val) 0.6480 | *\n",
      "Epoch 00007 | Time(s) 0.2802 | Loss(train) 1.4653 | Acc(train) 0.6786 | Loss(val) 1.4035 | Acc(val) 0.6760 | *\n",
      "Epoch 00008 | Time(s) 0.2511 | Loss(train) 1.3465 | Acc(train) 0.6857 | Loss(val) 1.2891 | Acc(val) 0.7120 | *\n",
      "Epoch 00009 | Time(s) 0.2947 | Loss(train) 1.1969 | Acc(train) 0.7286 | Loss(val) 1.1716 | Acc(val) 0.7320 | *\n",
      "Epoch 00010 | Time(s) 0.2838 | Loss(train) 1.0704 | Acc(train) 0.8000 | Loss(val) 1.0586 | Acc(val) 0.7340 | *\n",
      "Epoch 00011 | Time(s) 0.2651 | Loss(train) 0.9584 | Acc(train) 0.8286 | Loss(val) 0.9589 | Acc(val) 0.7500 | *\n",
      "Epoch 00012 | Time(s) 0.2657 | Loss(train) 0.8482 | Acc(train) 0.8429 | Loss(val) 0.8761 | Acc(val) 0.7640 | *\n",
      "Epoch 00013 | Time(s) 0.2942 | Loss(train) 0.7610 | Acc(train) 0.8357 | Loss(val) 0.8112 | Acc(val) 0.7600 | \n",
      "Epoch 00014 | Time(s) 0.2833 | Loss(train) 0.6902 | Acc(train) 0.8357 | Loss(val) 0.7655 | Acc(val) 0.7620 | \n",
      "Epoch 00015 | Time(s) 0.2686 | Loss(train) 0.6315 | Acc(train) 0.8214 | Loss(val) 0.7348 | Acc(val) 0.7600 | \n",
      "Epoch 00016 | Time(s) 0.2662 | Loss(train) 0.5891 | Acc(train) 0.8286 | Loss(val) 0.7116 | Acc(val) 0.7720 | *\n",
      "Epoch 00017 | Time(s) 0.2515 | Loss(train) 0.5470 | Acc(train) 0.8357 | Loss(val) 0.6989 | Acc(val) 0.7700 | \n",
      "Epoch 00018 | Time(s) 0.2739 | Loss(train) 0.5214 | Acc(train) 0.8286 | Loss(val) 0.6903 | Acc(val) 0.7680 | \n",
      "Epoch 00019 | Time(s) 0.2595 | Loss(train) 0.5056 | Acc(train) 0.8500 | Loss(val) 0.6852 | Acc(val) 0.7680 | \n",
      "Epoch 00020 | Time(s) 0.2835 | Loss(train) 0.4737 | Acc(train) 0.8429 | Loss(val) 0.6815 | Acc(val) 0.7660 | \n",
      "Epoch 00021 | Time(s) 0.2601 | Loss(train) 0.4600 | Acc(train) 0.8214 | Loss(val) 0.6869 | Acc(val) 0.7680 | \n",
      "Epoch 00022 | Time(s) 0.2836 | Loss(train) 0.4377 | Acc(train) 0.8500 | Loss(val) 0.6882 | Acc(val) 0.7640 | \n",
      "Epoch 00023 | Time(s) 0.2624 | Loss(train) 0.4284 | Acc(train) 0.8500 | Loss(val) 0.6952 | Acc(val) 0.7720 | \n",
      "Epoch 00024 | Time(s) 0.2830 | Loss(train) 0.4061 | Acc(train) 0.8714 | Loss(val) 0.6936 | Acc(val) 0.7780 | *\n",
      "Epoch 00025 | Time(s) 0.2621 | Loss(train) 0.3905 | Acc(train) 0.8714 | Loss(val) 0.6852 | Acc(val) 0.7880 | *\n",
      "Epoch 00026 | Time(s) 0.2822 | Loss(train) 0.3795 | Acc(train) 0.9071 | Loss(val) 0.6895 | Acc(val) 0.7840 | \n",
      "Epoch 00027 | Time(s) 0.2654 | Loss(train) 0.3627 | Acc(train) 0.8857 | Loss(val) 0.7023 | Acc(val) 0.7820 | \n",
      "Epoch 00028 | Time(s) 0.2668 | Loss(train) 0.3534 | Acc(train) 0.8643 | Loss(val) 0.7204 | Acc(val) 0.7800 | \n",
      "Epoch 00029 | Time(s) 0.2689 | Loss(train) 0.3389 | Acc(train) 0.8929 | Loss(val) 0.7360 | Acc(val) 0.7720 | \n",
      "Epoch 00030 | Time(s) 0.2639 | Loss(train) 0.3377 | Acc(train) 0.9071 | Loss(val) 0.7326 | Acc(val) 0.7800 | \n",
      "Epoch 00031 | Time(s) 0.2828 | Loss(train) 0.3305 | Acc(train) 0.9071 | Loss(val) 0.7154 | Acc(val) 0.7840 | \n",
      "Epoch 00032 | Time(s) 0.2682 | Loss(train) 0.3268 | Acc(train) 0.9000 | Loss(val) 0.7065 | Acc(val) 0.7880 | \n",
      "Epoch 00033 | Time(s) 0.2629 | Loss(train) 0.3267 | Acc(train) 0.9000 | Loss(val) 0.7115 | Acc(val) 0.7840 | \n",
      "Epoch 00034 | Time(s) 0.2864 | Loss(train) 0.3157 | Acc(train) 0.8786 | Loss(val) 0.7212 | Acc(val) 0.7780 | \n",
      "Epoch 00035 | Time(s) 0.2666 | Loss(train) 0.3035 | Acc(train) 0.8929 | Loss(val) 0.7432 | Acc(val) 0.7680 | \n",
      "Epoch 00036 | Time(s) 0.2924 | Loss(train) 0.2963 | Acc(train) 0.9000 | Loss(val) 0.7472 | Acc(val) 0.7700 | \n",
      "Epoch 00037 | Time(s) 0.2777 | Loss(train) 0.2897 | Acc(train) 0.9000 | Loss(val) 0.7252 | Acc(val) 0.7780 | \n",
      "Epoch 00038 | Time(s) 0.2693 | Loss(train) 0.2766 | Acc(train) 0.9214 | Loss(val) 0.7089 | Acc(val) 0.7740 | \n",
      "Epoch 00039 | Time(s) 0.2602 | Loss(train) 0.2749 | Acc(train) 0.9143 | Loss(val) 0.7055 | Acc(val) 0.7840 | \n",
      "Epoch 00040 | Time(s) 0.2910 | Loss(train) 0.2740 | Acc(train) 0.9143 | Loss(val) 0.7120 | Acc(val) 0.7840 | \n",
      "Epoch 00041 | Time(s) 0.2731 | Loss(train) 0.2751 | Acc(train) 0.9071 | Loss(val) 0.7198 | Acc(val) 0.7820 | \n",
      "Epoch 00042 | Time(s) 0.2809 | Loss(train) 0.2626 | Acc(train) 0.9071 | Loss(val) 0.7307 | Acc(val) 0.7800 | \n",
      "Epoch 00043 | Time(s) 0.2804 | Loss(train) 0.2541 | Acc(train) 0.9143 | Loss(val) 0.7435 | Acc(val) 0.7820 | \n",
      "Epoch 00044 | Time(s) 0.2764 | Loss(train) 0.2535 | Acc(train) 0.9071 | Loss(val) 0.7447 | Acc(val) 0.7820 | \n",
      "Epoch 00045 | Time(s) 0.2571 | Loss(train) 0.2534 | Acc(train) 0.9214 | Loss(val) 0.7287 | Acc(val) 0.7880 | \n",
      "Epoch 00046 | Time(s) 0.2843 | Loss(train) 0.2503 | Acc(train) 0.9214 | Loss(val) 0.7294 | Acc(val) 0.7860 | \n",
      "Epoch 00047 | Time(s) 0.2801 | Loss(train) 0.2413 | Acc(train) 0.9286 | Loss(val) 0.7316 | Acc(val) 0.7840 | \n",
      "Epoch 00048 | Time(s) 0.2803 | Loss(train) 0.2501 | Acc(train) 0.9286 | Loss(val) 0.7411 | Acc(val) 0.7860 | \n",
      "Epoch 00049 | Time(s) 0.2822 | Loss(train) 0.2333 | Acc(train) 0.9214 | Loss(val) 0.7633 | Acc(val) 0.7780 | \n",
      "Epoch 00050 | Time(s) 0.2753 | Loss(train) 0.2301 | Acc(train) 0.9214 | Loss(val) 0.7612 | Acc(val) 0.7820 | \n",
      "Epoch 00051 | Time(s) 0.2874 | Loss(train) 0.2325 | Acc(train) 0.9214 | Loss(val) 0.7424 | Acc(val) 0.7860 | \n",
      "Epoch 00052 | Time(s) 0.2843 | Loss(train) 0.2269 | Acc(train) 0.9071 | Loss(val) 0.7325 | Acc(val) 0.7880 | \n",
      "Epoch 00053 | Time(s) 0.2783 | Loss(train) 0.2358 | Acc(train) 0.9071 | Loss(val) 0.7273 | Acc(val) 0.7900 | *\n",
      "Epoch 00054 | Time(s) 0.2754 | Loss(train) 0.2253 | Acc(train) 0.9429 | Loss(val) 0.7387 | Acc(val) 0.7860 | \n",
      "Epoch 00055 | Time(s) 0.2903 | Loss(train) 0.2251 | Acc(train) 0.9214 | Loss(val) 0.7659 | Acc(val) 0.7840 | \n",
      "Epoch 00056 | Time(s) 0.2900 | Loss(train) 0.2270 | Acc(train) 0.9286 | Loss(val) 0.7802 | Acc(val) 0.7800 | \n",
      "Epoch 00057 | Time(s) 0.2923 | Loss(train) 0.2270 | Acc(train) 0.9214 | Loss(val) 0.7673 | Acc(val) 0.7820 | \n",
      "Epoch 00058 | Time(s) 0.2889 | Loss(train) 0.2159 | Acc(train) 0.9357 | Loss(val) 0.7586 | Acc(val) 0.7800 | \n",
      "Epoch 00059 | Time(s) 0.2805 | Loss(train) 0.2210 | Acc(train) 0.9214 | Loss(val) 0.7561 | Acc(val) 0.7820 | \n",
      "Epoch 00060 | Time(s) 0.2803 | Loss(train) 0.2068 | Acc(train) 0.9500 | Loss(val) 0.7592 | Acc(val) 0.7800 | \n",
      "Epoch 00061 | Time(s) 0.2826 | Loss(train) 0.2073 | Acc(train) 0.9286 | Loss(val) 0.7689 | Acc(val) 0.7760 | \n",
      "Epoch 00062 | Time(s) 0.2888 | Loss(train) 0.2156 | Acc(train) 0.9286 | Loss(val) 0.7781 | Acc(val) 0.7740 | \n",
      "Epoch 00063 | Time(s) 0.2882 | Loss(train) 0.2032 | Acc(train) 0.9143 | Loss(val) 0.7958 | Acc(val) 0.7740 | \n",
      "Epoch 00064 | Time(s) 0.2732 | Loss(train) 0.1924 | Acc(train) 0.9357 | Loss(val) 0.8083 | Acc(val) 0.7700 | \n",
      "Epoch 00065 | Time(s) 0.2861 | Loss(train) 0.1962 | Acc(train) 0.9286 | Loss(val) 0.8155 | Acc(val) 0.7720 | \n",
      "Epoch 00066 | Time(s) 0.2770 | Loss(train) 0.1809 | Acc(train) 0.9429 | Loss(val) 0.8132 | Acc(val) 0.7780 | \n",
      "Epoch 00067 | Time(s) 0.2904 | Loss(train) 0.1753 | Acc(train) 0.9500 | Loss(val) 0.8116 | Acc(val) 0.7700 | \n",
      "Epoch 00068 | Time(s) 0.3111 | Loss(train) 0.1695 | Acc(train) 0.9500 | Loss(val) 0.8121 | Acc(val) 0.7680 | \n",
      "Epoch 00069 | Time(s) 0.2803 | Loss(train) 0.1764 | Acc(train) 0.9429 | Loss(val) 0.8190 | Acc(val) 0.7700 | \n",
      "Epoch 00070 | Time(s) 0.2808 | Loss(train) 0.1698 | Acc(train) 0.9429 | Loss(val) 0.8259 | Acc(val) 0.7720 | \n",
      "Epoch 00071 | Time(s) 0.2768 | Loss(train) 0.1628 | Acc(train) 0.9500 | Loss(val) 0.8400 | Acc(val) 0.7720 | \n",
      "Epoch 00072 | Time(s) 0.2773 | Loss(train) 0.1509 | Acc(train) 0.9500 | Loss(val) 0.8604 | Acc(val) 0.7700 | \n",
      "Epoch 00073 | Time(s) 0.2764 | Loss(train) 0.1529 | Acc(train) 0.9500 | Loss(val) 0.8720 | Acc(val) 0.7660 | \n",
      "Epoch 00074 | Time(s) 0.2834 | Loss(train) 0.1468 | Acc(train) 0.9571 | Loss(val) 0.8716 | Acc(val) 0.7580 | \n",
      "Epoch 00075 | Time(s) 0.2831 | Loss(train) 0.1426 | Acc(train) 0.9643 | Loss(val) 0.8649 | Acc(val) 0.7560 | \n",
      "Epoch 00076 | Time(s) 0.2921 | Loss(train) 0.1421 | Acc(train) 0.9643 | Loss(val) 0.8546 | Acc(val) 0.7560 | \n",
      "Epoch 00077 | Time(s) 0.2964 | Loss(train) 0.1344 | Acc(train) 0.9643 | Loss(val) 0.8542 | Acc(val) 0.7640 | \n",
      "Epoch 00078 | Time(s) 0.2908 | Loss(train) 0.1275 | Acc(train) 0.9786 | Loss(val) 0.8683 | Acc(val) 0.7660 | \n",
      "Epoch 00079 | Time(s) 0.2815 | Loss(train) 0.1230 | Acc(train) 0.9714 | Loss(val) 0.8865 | Acc(val) 0.7680 | \n",
      "Epoch 00080 | Time(s) 0.2783 | Loss(train) 0.1154 | Acc(train) 0.9714 | Loss(val) 0.9029 | Acc(val) 0.7640 | \n",
      "Epoch 00081 | Time(s) 0.2784 | Loss(train) 0.1139 | Acc(train) 0.9786 | Loss(val) 0.9090 | Acc(val) 0.7560 | \n",
      "Epoch 00082 | Time(s) 0.2763 | Loss(train) 0.1133 | Acc(train) 0.9786 | Loss(val) 0.9005 | Acc(val) 0.7580 | \n",
      "Epoch 00083 | Time(s) 0.2862 | Loss(train) 0.1099 | Acc(train) 0.9786 | Loss(val) 0.8859 | Acc(val) 0.7560 | \n",
      "Epoch 00084 | Time(s) 0.2883 | Loss(train) 0.1120 | Acc(train) 0.9857 | Loss(val) 0.8859 | Acc(val) 0.7580 | \n",
      "Epoch 00085 | Time(s) 0.2816 | Loss(train) 0.1063 | Acc(train) 0.9786 | Loss(val) 0.8910 | Acc(val) 0.7620 | \n",
      "Epoch 00086 | Time(s) 0.2876 | Loss(train) 0.0997 | Acc(train) 0.9786 | Loss(val) 0.9053 | Acc(val) 0.7620 | \n",
      "Epoch 00087 | Time(s) 0.2985 | Loss(train) 0.0963 | Acc(train) 0.9786 | Loss(val) 0.9153 | Acc(val) 0.7600 | \n",
      "Epoch 00088 | Time(s) 0.2791 | Loss(train) 0.0954 | Acc(train) 0.9786 | Loss(val) 0.9312 | Acc(val) 0.7600 | \n",
      "Epoch 00089 | Time(s) 0.2714 | Loss(train) 0.0928 | Acc(train) 0.9857 | Loss(val) 0.9482 | Acc(val) 0.7580 | \n",
      "Epoch 00090 | Time(s) 0.2986 | Loss(train) 0.0923 | Acc(train) 0.9857 | Loss(val) 0.9693 | Acc(val) 0.7580 | \n",
      "Epoch 00091 | Time(s) 0.2832 | Loss(train) 0.0963 | Acc(train) 0.9786 | Loss(val) 0.9769 | Acc(val) 0.7520 | \n",
      "Epoch 00092 | Time(s) 0.2825 | Loss(train) 0.0859 | Acc(train) 0.9857 | Loss(val) 0.9844 | Acc(val) 0.7560 | \n",
      "Epoch 00093 | Time(s) 0.2827 | Loss(train) 0.0871 | Acc(train) 0.9786 | Loss(val) 0.9843 | Acc(val) 0.7540 | \n",
      "Epoch 00094 | Time(s) 0.2832 | Loss(train) 0.0830 | Acc(train) 0.9857 | Loss(val) 0.9759 | Acc(val) 0.7540 | \n",
      "Epoch 00095 | Time(s) 0.2822 | Loss(train) 0.0818 | Acc(train) 0.9857 | Loss(val) 0.9746 | Acc(val) 0.7540 | \n",
      "Epoch 00096 | Time(s) 0.2614 | Loss(train) 0.0859 | Acc(train) 0.9857 | Loss(val) 0.9932 | Acc(val) 0.7580 | \n",
      "Epoch 00097 | Time(s) 0.2980 | Loss(train) 0.0811 | Acc(train) 0.9857 | Loss(val) 1.0228 | Acc(val) 0.7560 | \n",
      "Epoch 00098 | Time(s) 0.2678 | Loss(train) 0.0740 | Acc(train) 0.9929 | Loss(val) 1.0446 | Acc(val) 0.7540 | \n",
      "Epoch 00099 | Time(s) 0.2810 | Loss(train) 0.0762 | Acc(train) 0.9929 | Loss(val) 1.0560 | Acc(val) 0.7460 | \n",
      "Epoch 00100 | Time(s) 0.2837 | Loss(train) 0.0771 | Acc(train) 0.9857 | Loss(val) 1.0547 | Acc(val) 0.7480 | \n",
      "Optimization Finished!\n",
      "Time(s): 14.5357\n",
      "Loss(test) 0.5827 | Acc(test) 0.8280\n",
      "Exp 1/3\n",
      "Epoch 00001 | Time(s) 0.2871 | Loss(train) 1.9495 | Acc(train) 0.1429 | Loss(val) 1.9192 | Acc(val) 0.3060 | *\n",
      "Epoch 00002 | Time(s) 0.2670 | Loss(train) 1.9121 | Acc(train) 0.3786 | Loss(val) 1.8622 | Acc(val) 0.5800 | *\n",
      "Epoch 00003 | Time(s) 0.2704 | Loss(train) 1.8513 | Acc(train) 0.6214 | Loss(val) 1.7937 | Acc(val) 0.6100 | *\n",
      "Epoch 00004 | Time(s) 0.3051 | Loss(train) 1.7813 | Acc(train) 0.6143 | Loss(val) 1.7142 | Acc(val) 0.6520 | *\n",
      "Epoch 00005 | Time(s) 0.2730 | Loss(train) 1.6968 | Acc(train) 0.6571 | Loss(val) 1.6238 | Acc(val) 0.6840 | *\n",
      "Epoch 00006 | Time(s) 0.2742 | Loss(train) 1.5978 | Acc(train) 0.6786 | Loss(val) 1.5233 | Acc(val) 0.7020 | *\n",
      "Epoch 00007 | Time(s) 0.2653 | Loss(train) 1.4884 | Acc(train) 0.7286 | Loss(val) 1.4155 | Acc(val) 0.7340 | *\n",
      "Epoch 00008 | Time(s) 0.3026 | Loss(train) 1.3716 | Acc(train) 0.7857 | Loss(val) 1.3029 | Acc(val) 0.7360 | *\n",
      "Epoch 00009 | Time(s) 0.2775 | Loss(train) 1.2414 | Acc(train) 0.8286 | Loss(val) 1.1903 | Acc(val) 0.7460 | *\n",
      "Epoch 00010 | Time(s) 0.2702 | Loss(train) 1.1187 | Acc(train) 0.8429 | Loss(val) 1.0813 | Acc(val) 0.7540 | *\n",
      "Epoch 00011 | Time(s) 0.2841 | Loss(train) 0.9928 | Acc(train) 0.8429 | Loss(val) 0.9829 | Acc(val) 0.7520 | \n",
      "Epoch 00012 | Time(s) 0.2674 | Loss(train) 0.8871 | Acc(train) 0.8286 | Loss(val) 0.8989 | Acc(val) 0.7560 | *\n",
      "Epoch 00013 | Time(s) 0.2964 | Loss(train) 0.7974 | Acc(train) 0.8357 | Loss(val) 0.8289 | Acc(val) 0.7620 | *\n",
      "Epoch 00014 | Time(s) 0.2655 | Loss(train) 0.7238 | Acc(train) 0.8143 | Loss(val) 0.7754 | Acc(val) 0.7640 | *\n",
      "Epoch 00015 | Time(s) 0.2865 | Loss(train) 0.6663 | Acc(train) 0.8286 | Loss(val) 0.7408 | Acc(val) 0.7660 | *\n",
      "Epoch 00016 | Time(s) 0.2798 | Loss(train) 0.6177 | Acc(train) 0.8429 | Loss(val) 0.7257 | Acc(val) 0.7620 | \n",
      "Epoch 00017 | Time(s) 0.2790 | Loss(train) 0.5830 | Acc(train) 0.8429 | Loss(val) 0.7156 | Acc(val) 0.7640 | \n",
      "Epoch 00018 | Time(s) 0.2832 | Loss(train) 0.5459 | Acc(train) 0.8429 | Loss(val) 0.7025 | Acc(val) 0.7680 | *\n",
      "Epoch 00019 | Time(s) 0.2811 | Loss(train) 0.5225 | Acc(train) 0.8357 | Loss(val) 0.6928 | Acc(val) 0.7680 | \n",
      "Epoch 00020 | Time(s) 0.2702 | Loss(train) 0.5006 | Acc(train) 0.8500 | Loss(val) 0.6861 | Acc(val) 0.7700 | *\n",
      "Epoch 00021 | Time(s) 0.2836 | Loss(train) 0.4760 | Acc(train) 0.8429 | Loss(val) 0.6866 | Acc(val) 0.7720 | *\n",
      "Epoch 00022 | Time(s) 0.2760 | Loss(train) 0.4583 | Acc(train) 0.8500 | Loss(val) 0.6916 | Acc(val) 0.7700 | \n",
      "Epoch 00023 | Time(s) 0.2935 | Loss(train) 0.4391 | Acc(train) 0.8571 | Loss(val) 0.6973 | Acc(val) 0.7780 | *\n",
      "Epoch 00024 | Time(s) 0.2784 | Loss(train) 0.4266 | Acc(train) 0.8714 | Loss(val) 0.6981 | Acc(val) 0.7720 | \n",
      "Epoch 00025 | Time(s) 0.2782 | Loss(train) 0.4045 | Acc(train) 0.8786 | Loss(val) 0.6997 | Acc(val) 0.7760 | \n",
      "Epoch 00026 | Time(s) 0.2893 | Loss(train) 0.3886 | Acc(train) 0.8714 | Loss(val) 0.7084 | Acc(val) 0.7780 | \n",
      "Epoch 00027 | Time(s) 0.2838 | Loss(train) 0.3719 | Acc(train) 0.8929 | Loss(val) 0.7123 | Acc(val) 0.7800 | *\n",
      "Epoch 00028 | Time(s) 0.2790 | Loss(train) 0.3757 | Acc(train) 0.8929 | Loss(val) 0.7106 | Acc(val) 0.7780 | \n",
      "Epoch 00029 | Time(s) 0.2791 | Loss(train) 0.3471 | Acc(train) 0.9000 | Loss(val) 0.7066 | Acc(val) 0.7780 | \n",
      "Epoch 00030 | Time(s) 0.2798 | Loss(train) 0.3438 | Acc(train) 0.8929 | Loss(val) 0.6988 | Acc(val) 0.7800 | \n",
      "Epoch 00031 | Time(s) 0.2918 | Loss(train) 0.3370 | Acc(train) 0.9000 | Loss(val) 0.6903 | Acc(val) 0.7840 | *\n",
      "Epoch 00032 | Time(s) 0.2871 | Loss(train) 0.3312 | Acc(train) 0.9000 | Loss(val) 0.6955 | Acc(val) 0.7880 | *\n",
      "Epoch 00033 | Time(s) 0.2760 | Loss(train) 0.3187 | Acc(train) 0.8929 | Loss(val) 0.7137 | Acc(val) 0.7840 | \n",
      "Epoch 00034 | Time(s) 0.2787 | Loss(train) 0.3202 | Acc(train) 0.8929 | Loss(val) 0.7297 | Acc(val) 0.7800 | \n",
      "Epoch 00035 | Time(s) 0.2746 | Loss(train) 0.3067 | Acc(train) 0.9143 | Loss(val) 0.7360 | Acc(val) 0.7760 | \n",
      "Epoch 00036 | Time(s) 0.3014 | Loss(train) 0.3028 | Acc(train) 0.9071 | Loss(val) 0.7173 | Acc(val) 0.7820 | \n",
      "Epoch 00037 | Time(s) 0.2833 | Loss(train) 0.2961 | Acc(train) 0.9214 | Loss(val) 0.6980 | Acc(val) 0.7840 | \n",
      "Epoch 00038 | Time(s) 0.2704 | Loss(train) 0.2845 | Acc(train) 0.9286 | Loss(val) 0.6925 | Acc(val) 0.7860 | \n",
      "Epoch 00039 | Time(s) 0.2778 | Loss(train) 0.2872 | Acc(train) 0.9071 | Loss(val) 0.7009 | Acc(val) 0.7820 | \n",
      "Epoch 00040 | Time(s) 0.2863 | Loss(train) 0.2875 | Acc(train) 0.9000 | Loss(val) 0.7219 | Acc(val) 0.7740 | \n",
      "Epoch 00041 | Time(s) 0.2751 | Loss(train) 0.2695 | Acc(train) 0.9071 | Loss(val) 0.7387 | Acc(val) 0.7720 | \n",
      "Epoch 00042 | Time(s) 0.2972 | Loss(train) 0.2641 | Acc(train) 0.9286 | Loss(val) 0.7388 | Acc(val) 0.7760 | \n",
      "Epoch 00043 | Time(s) 0.2996 | Loss(train) 0.2617 | Acc(train) 0.9286 | Loss(val) 0.7273 | Acc(val) 0.7800 | \n",
      "Epoch 00044 | Time(s) 0.2869 | Loss(train) 0.2593 | Acc(train) 0.9214 | Loss(val) 0.7208 | Acc(val) 0.7840 | \n",
      "Epoch 00045 | Time(s) 0.2913 | Loss(train) 0.2536 | Acc(train) 0.9214 | Loss(val) 0.7277 | Acc(val) 0.7860 | \n",
      "Epoch 00046 | Time(s) 0.2792 | Loss(train) 0.2429 | Acc(train) 0.9143 | Loss(val) 0.7296 | Acc(val) 0.7820 | \n",
      "Epoch 00047 | Time(s) 0.2718 | Loss(train) 0.2388 | Acc(train) 0.9143 | Loss(val) 0.7359 | Acc(val) 0.7840 | \n",
      "Epoch 00048 | Time(s) 0.3006 | Loss(train) 0.2356 | Acc(train) 0.9214 | Loss(val) 0.7392 | Acc(val) 0.7840 | \n",
      "Epoch 00049 | Time(s) 0.2922 | Loss(train) 0.2332 | Acc(train) 0.9143 | Loss(val) 0.7483 | Acc(val) 0.7860 | \n",
      "Epoch 00050 | Time(s) 0.2888 | Loss(train) 0.2343 | Acc(train) 0.9214 | Loss(val) 0.7566 | Acc(val) 0.7860 | \n",
      "Epoch 00051 | Time(s) 0.2853 | Loss(train) 0.2282 | Acc(train) 0.9357 | Loss(val) 0.7565 | Acc(val) 0.7860 | \n",
      "Epoch 00052 | Time(s) 0.2757 | Loss(train) 0.2353 | Acc(train) 0.9286 | Loss(val) 0.7547 | Acc(val) 0.7840 | \n",
      "Epoch 00053 | Time(s) 0.3033 | Loss(train) 0.2234 | Acc(train) 0.9214 | Loss(val) 0.7565 | Acc(val) 0.7840 | \n",
      "Epoch 00054 | Time(s) 0.2636 | Loss(train) 0.2255 | Acc(train) 0.9214 | Loss(val) 0.7586 | Acc(val) 0.7820 | \n",
      "Epoch 00055 | Time(s) 0.2937 | Loss(train) 0.2235 | Acc(train) 0.9143 | Loss(val) 0.7694 | Acc(val) 0.7920 | *\n",
      "Epoch 00056 | Time(s) 0.2801 | Loss(train) 0.2245 | Acc(train) 0.9286 | Loss(val) 0.7792 | Acc(val) 0.7840 | \n",
      "Epoch 00057 | Time(s) 0.2831 | Loss(train) 0.2190 | Acc(train) 0.9286 | Loss(val) 0.7879 | Acc(val) 0.7880 | \n",
      "Epoch 00058 | Time(s) 0.2924 | Loss(train) 0.2106 | Acc(train) 0.9214 | Loss(val) 0.7750 | Acc(val) 0.7820 | \n",
      "Epoch 00059 | Time(s) 0.2845 | Loss(train) 0.2079 | Acc(train) 0.9286 | Loss(val) 0.7720 | Acc(val) 0.7740 | \n",
      "Epoch 00060 | Time(s) 0.3062 | Loss(train) 0.2020 | Acc(train) 0.9286 | Loss(val) 0.7800 | Acc(val) 0.7780 | \n",
      "Epoch 00061 | Time(s) 0.2872 | Loss(train) 0.2045 | Acc(train) 0.9286 | Loss(val) 0.7946 | Acc(val) 0.7720 | \n",
      "Epoch 00062 | Time(s) 0.2834 | Loss(train) 0.1970 | Acc(train) 0.9286 | Loss(val) 0.8089 | Acc(val) 0.7720 | \n",
      "Epoch 00063 | Time(s) 0.2740 | Loss(train) 0.1932 | Acc(train) 0.9286 | Loss(val) 0.8199 | Acc(val) 0.7780 | \n",
      "Epoch 00064 | Time(s) 0.2814 | Loss(train) 0.1868 | Acc(train) 0.9500 | Loss(val) 0.8093 | Acc(val) 0.7720 | \n",
      "Epoch 00065 | Time(s) 0.2780 | Loss(train) 0.1869 | Acc(train) 0.9429 | Loss(val) 0.7987 | Acc(val) 0.7720 | \n",
      "Epoch 00066 | Time(s) 0.2815 | Loss(train) 0.1793 | Acc(train) 0.9429 | Loss(val) 0.8046 | Acc(val) 0.7700 | \n",
      "Epoch 00067 | Time(s) 0.2709 | Loss(train) 0.1729 | Acc(train) 0.9500 | Loss(val) 0.8164 | Acc(val) 0.7740 | \n",
      "Epoch 00068 | Time(s) 0.2744 | Loss(train) 0.1694 | Acc(train) 0.9500 | Loss(val) 0.8338 | Acc(val) 0.7680 | \n",
      "Epoch 00069 | Time(s) 0.2942 | Loss(train) 0.1641 | Acc(train) 0.9571 | Loss(val) 0.8456 | Acc(val) 0.7700 | \n",
      "Epoch 00070 | Time(s) 0.2709 | Loss(train) 0.1636 | Acc(train) 0.9643 | Loss(val) 0.8425 | Acc(val) 0.7720 | \n",
      "Epoch 00071 | Time(s) 0.2776 | Loss(train) 0.1478 | Acc(train) 0.9571 | Loss(val) 0.8344 | Acc(val) 0.7780 | \n",
      "Epoch 00072 | Time(s) 0.2869 | Loss(train) 0.1491 | Acc(train) 0.9500 | Loss(val) 0.8411 | Acc(val) 0.7780 | \n",
      "Epoch 00073 | Time(s) 0.2796 | Loss(train) 0.1438 | Acc(train) 0.9571 | Loss(val) 0.8630 | Acc(val) 0.7740 | \n",
      "Epoch 00074 | Time(s) 0.2860 | Loss(train) 0.1381 | Acc(train) 0.9571 | Loss(val) 0.8732 | Acc(val) 0.7720 | \n",
      "Epoch 00075 | Time(s) 0.2897 | Loss(train) 0.1339 | Acc(train) 0.9643 | Loss(val) 0.8759 | Acc(val) 0.7720 | \n",
      "Epoch 00076 | Time(s) 0.2868 | Loss(train) 0.1279 | Acc(train) 0.9786 | Loss(val) 0.8674 | Acc(val) 0.7740 | \n",
      "Epoch 00077 | Time(s) 0.2771 | Loss(train) 0.1247 | Acc(train) 0.9786 | Loss(val) 0.8620 | Acc(val) 0.7720 | \n",
      "Epoch 00078 | Time(s) 0.2753 | Loss(train) 0.1181 | Acc(train) 0.9786 | Loss(val) 0.8638 | Acc(val) 0.7740 | \n",
      "Epoch 00079 | Time(s) 0.2824 | Loss(train) 0.1146 | Acc(train) 0.9786 | Loss(val) 0.8602 | Acc(val) 0.7760 | \n",
      "Epoch 00080 | Time(s) 0.2818 | Loss(train) 0.1131 | Acc(train) 0.9714 | Loss(val) 0.8617 | Acc(val) 0.7700 | \n",
      "Epoch 00081 | Time(s) 0.2652 | Loss(train) 0.1129 | Acc(train) 0.9786 | Loss(val) 0.8831 | Acc(val) 0.7620 | \n",
      "Epoch 00082 | Time(s) 0.2831 | Loss(train) 0.1079 | Acc(train) 0.9786 | Loss(val) 0.9137 | Acc(val) 0.7620 | \n",
      "Epoch 00083 | Time(s) 0.2866 | Loss(train) 0.1067 | Acc(train) 0.9857 | Loss(val) 0.9392 | Acc(val) 0.7620 | \n",
      "Epoch 00084 | Time(s) 0.3005 | Loss(train) 0.1069 | Acc(train) 0.9786 | Loss(val) 0.9395 | Acc(val) 0.7640 | \n",
      "Epoch 00085 | Time(s) 0.2777 | Loss(train) 0.0989 | Acc(train) 0.9714 | Loss(val) 0.9155 | Acc(val) 0.7680 | \n",
      "Epoch 00086 | Time(s) 0.2698 | Loss(train) 0.0976 | Acc(train) 0.9786 | Loss(val) 0.8919 | Acc(val) 0.7720 | \n",
      "Epoch 00087 | Time(s) 0.2928 | Loss(train) 0.0936 | Acc(train) 0.9786 | Loss(val) 0.8938 | Acc(val) 0.7620 | \n",
      "Epoch 00088 | Time(s) 0.2737 | Loss(train) 0.0942 | Acc(train) 0.9786 | Loss(val) 0.9162 | Acc(val) 0.7620 | \n",
      "Epoch 00089 | Time(s) 0.2880 | Loss(train) 0.0944 | Acc(train) 0.9786 | Loss(val) 0.9509 | Acc(val) 0.7540 | \n",
      "Epoch 00090 | Time(s) 0.2868 | Loss(train) 0.0949 | Acc(train) 0.9857 | Loss(val) 0.9962 | Acc(val) 0.7580 | \n",
      "Epoch 00091 | Time(s) 0.2759 | Loss(train) 0.0903 | Acc(train) 0.9857 | Loss(val) 1.0176 | Acc(val) 0.7580 | \n",
      "Epoch 00092 | Time(s) 0.2929 | Loss(train) 0.0916 | Acc(train) 0.9714 | Loss(val) 1.0111 | Acc(val) 0.7560 | \n",
      "Epoch 00093 | Time(s) 0.2845 | Loss(train) 0.0893 | Acc(train) 0.9786 | Loss(val) 0.9939 | Acc(val) 0.7540 | \n",
      "Epoch 00094 | Time(s) 0.2927 | Loss(train) 0.0875 | Acc(train) 0.9857 | Loss(val) 0.9857 | Acc(val) 0.7520 | \n",
      "Epoch 00095 | Time(s) 0.2897 | Loss(train) 0.0836 | Acc(train) 0.9929 | Loss(val) 0.9876 | Acc(val) 0.7520 | \n",
      "Epoch 00096 | Time(s) 0.2926 | Loss(train) 0.0870 | Acc(train) 0.9786 | Loss(val) 1.0026 | Acc(val) 0.7580 | \n",
      "Epoch 00097 | Time(s) 0.2975 | Loss(train) 0.0761 | Acc(train) 0.9857 | Loss(val) 1.0284 | Acc(val) 0.7600 | \n",
      "Epoch 00098 | Time(s) 0.2817 | Loss(train) 0.0897 | Acc(train) 0.9714 | Loss(val) 1.0454 | Acc(val) 0.7560 | \n",
      "Epoch 00099 | Time(s) 0.2858 | Loss(train) 0.0813 | Acc(train) 0.9714 | Loss(val) 1.0533 | Acc(val) 0.7560 | \n",
      "Epoch 00100 | Time(s) 0.2664 | Loss(train) 0.0788 | Acc(train) 0.9786 | Loss(val) 1.0570 | Acc(val) 0.7500 | \n",
      "Optimization Finished!\n",
      "Time(s): 15.5600\n",
      "Loss(test) 0.6057 | Acc(test) 0.8210\n",
      "Exp 2/3\n",
      "Epoch 00001 | Time(s) 0.3166 | Loss(train) 1.9473 | Acc(train) 0.1429 | Loss(val) 1.9342 | Acc(val) 0.2560 | *\n",
      "Epoch 00002 | Time(s) 0.2783 | Loss(train) 1.9119 | Acc(train) 0.3714 | Loss(val) 1.8895 | Acc(val) 0.3420 | *\n",
      "Epoch 00003 | Time(s) 0.3042 | Loss(train) 1.8513 | Acc(train) 0.4429 | Loss(val) 1.8385 | Acc(val) 0.4280 | *\n",
      "Epoch 00004 | Time(s) 0.2790 | Loss(train) 1.7848 | Acc(train) 0.5357 | Loss(val) 1.7722 | Acc(val) 0.4660 | *\n",
      "Epoch 00005 | Time(s) 0.2860 | Loss(train) 1.7034 | Acc(train) 0.5643 | Loss(val) 1.6908 | Acc(val) 0.4960 | *\n",
      "Epoch 00006 | Time(s) 0.2827 | Loss(train) 1.6091 | Acc(train) 0.6143 | Loss(val) 1.5973 | Acc(val) 0.5800 | *\n",
      "Epoch 00007 | Time(s) 0.2975 | Loss(train) 1.5080 | Acc(train) 0.6500 | Loss(val) 1.4931 | Acc(val) 0.6340 | *\n",
      "Epoch 00008 | Time(s) 0.2824 | Loss(train) 1.3965 | Acc(train) 0.6786 | Loss(val) 1.3852 | Acc(val) 0.6640 | *\n",
      "Epoch 00009 | Time(s) 0.2825 | Loss(train) 1.2792 | Acc(train) 0.6929 | Loss(val) 1.2755 | Acc(val) 0.6820 | *\n",
      "Epoch 00010 | Time(s) 0.2985 | Loss(train) 1.1595 | Acc(train) 0.6929 | Loss(val) 1.1700 | Acc(val) 0.6980 | *\n",
      "Epoch 00011 | Time(s) 0.2803 | Loss(train) 1.0475 | Acc(train) 0.7571 | Loss(val) 1.0711 | Acc(val) 0.7140 | *\n",
      "Epoch 00012 | Time(s) 0.2827 | Loss(train) 0.9474 | Acc(train) 0.7214 | Loss(val) 0.9814 | Acc(val) 0.7360 | *\n",
      "Epoch 00013 | Time(s) 0.2829 | Loss(train) 0.8621 | Acc(train) 0.7571 | Loss(val) 0.9044 | Acc(val) 0.7460 | *\n",
      "Epoch 00014 | Time(s) 0.2967 | Loss(train) 0.7814 | Acc(train) 0.8071 | Loss(val) 0.8389 | Acc(val) 0.7620 | *\n",
      "Epoch 00015 | Time(s) 0.2831 | Loss(train) 0.7184 | Acc(train) 0.8429 | Loss(val) 0.7863 | Acc(val) 0.7640 | *\n",
      "Epoch 00016 | Time(s) 0.2970 | Loss(train) 0.6671 | Acc(train) 0.8357 | Loss(val) 0.7493 | Acc(val) 0.7640 | \n",
      "Epoch 00017 | Time(s) 0.2831 | Loss(train) 0.6339 | Acc(train) 0.8571 | Loss(val) 0.7224 | Acc(val) 0.7700 | *\n",
      "Epoch 00018 | Time(s) 0.2823 | Loss(train) 0.5876 | Acc(train) 0.8571 | Loss(val) 0.7070 | Acc(val) 0.7660 | \n",
      "Epoch 00019 | Time(s) 0.2921 | Loss(train) 0.5622 | Acc(train) 0.8429 | Loss(val) 0.7009 | Acc(val) 0.7580 | \n",
      "Epoch 00020 | Time(s) 0.2717 | Loss(train) 0.5330 | Acc(train) 0.8286 | Loss(val) 0.7084 | Acc(val) 0.7640 | \n",
      "Epoch 00021 | Time(s) 0.2830 | Loss(train) 0.5046 | Acc(train) 0.8429 | Loss(val) 0.7144 | Acc(val) 0.7600 | \n",
      "Epoch 00022 | Time(s) 0.2823 | Loss(train) 0.4883 | Acc(train) 0.8429 | Loss(val) 0.7118 | Acc(val) 0.7560 | \n",
      "Epoch 00023 | Time(s) 0.2860 | Loss(train) 0.4652 | Acc(train) 0.8571 | Loss(val) 0.7019 | Acc(val) 0.7640 | \n",
      "Epoch 00024 | Time(s) 0.2812 | Loss(train) 0.4487 | Acc(train) 0.8643 | Loss(val) 0.6955 | Acc(val) 0.7660 | \n",
      "Epoch 00025 | Time(s) 0.2787 | Loss(train) 0.4279 | Acc(train) 0.8643 | Loss(val) 0.6963 | Acc(val) 0.7660 | \n",
      "Epoch 00026 | Time(s) 0.2977 | Loss(train) 0.4169 | Acc(train) 0.8857 | Loss(val) 0.7028 | Acc(val) 0.7680 | \n",
      "Epoch 00027 | Time(s) 0.2963 | Loss(train) 0.3976 | Acc(train) 0.8643 | Loss(val) 0.7139 | Acc(val) 0.7660 | \n",
      "Epoch 00028 | Time(s) 0.2700 | Loss(train) 0.3865 | Acc(train) 0.8857 | Loss(val) 0.7225 | Acc(val) 0.7660 | \n",
      "Epoch 00029 | Time(s) 0.2808 | Loss(train) 0.3740 | Acc(train) 0.8857 | Loss(val) 0.7229 | Acc(val) 0.7600 | \n",
      "Epoch 00030 | Time(s) 0.2837 | Loss(train) 0.3539 | Acc(train) 0.8857 | Loss(val) 0.7202 | Acc(val) 0.7600 | \n",
      "Epoch 00031 | Time(s) 0.2642 | Loss(train) 0.3555 | Acc(train) 0.8929 | Loss(val) 0.7129 | Acc(val) 0.7680 | \n",
      "Epoch 00032 | Time(s) 0.2830 | Loss(train) 0.3444 | Acc(train) 0.8857 | Loss(val) 0.7063 | Acc(val) 0.7720 | *\n",
      "Epoch 00033 | Time(s) 0.2981 | Loss(train) 0.3359 | Acc(train) 0.8929 | Loss(val) 0.7066 | Acc(val) 0.7740 | *\n",
      "Epoch 00034 | Time(s) 0.2661 | Loss(train) 0.3237 | Acc(train) 0.8929 | Loss(val) 0.7194 | Acc(val) 0.7720 | \n",
      "Epoch 00035 | Time(s) 0.2987 | Loss(train) 0.3177 | Acc(train) 0.8929 | Loss(val) 0.7355 | Acc(val) 0.7700 | \n",
      "Epoch 00036 | Time(s) 0.2920 | Loss(train) 0.3106 | Acc(train) 0.9071 | Loss(val) 0.7421 | Acc(val) 0.7720 | \n",
      "Epoch 00037 | Time(s) 0.2728 | Loss(train) 0.3092 | Acc(train) 0.9143 | Loss(val) 0.7394 | Acc(val) 0.7680 | \n",
      "Epoch 00038 | Time(s) 0.2949 | Loss(train) 0.2984 | Acc(train) 0.9000 | Loss(val) 0.7384 | Acc(val) 0.7700 | \n",
      "Epoch 00039 | Time(s) 0.3067 | Loss(train) 0.2956 | Acc(train) 0.9071 | Loss(val) 0.7356 | Acc(val) 0.7700 | \n",
      "Epoch 00040 | Time(s) 0.2872 | Loss(train) 0.2910 | Acc(train) 0.9000 | Loss(val) 0.7374 | Acc(val) 0.7760 | *\n",
      "Epoch 00041 | Time(s) 0.2729 | Loss(train) 0.2884 | Acc(train) 0.9071 | Loss(val) 0.7434 | Acc(val) 0.7720 | \n",
      "Epoch 00042 | Time(s) 0.2815 | Loss(train) 0.2828 | Acc(train) 0.9214 | Loss(val) 0.7422 | Acc(val) 0.7740 | \n",
      "Epoch 00043 | Time(s) 0.2895 | Loss(train) 0.2769 | Acc(train) 0.9214 | Loss(val) 0.7338 | Acc(val) 0.7800 | *\n",
      "Epoch 00044 | Time(s) 0.2727 | Loss(train) 0.2695 | Acc(train) 0.9214 | Loss(val) 0.7339 | Acc(val) 0.7840 | *\n",
      "Epoch 00045 | Time(s) 0.2831 | Loss(train) 0.2705 | Acc(train) 0.9143 | Loss(val) 0.7504 | Acc(val) 0.7720 | \n",
      "Epoch 00046 | Time(s) 0.2661 | Loss(train) 0.2608 | Acc(train) 0.9143 | Loss(val) 0.7684 | Acc(val) 0.7740 | \n",
      "Epoch 00047 | Time(s) 0.2834 | Loss(train) 0.2615 | Acc(train) 0.9214 | Loss(val) 0.7791 | Acc(val) 0.7700 | \n",
      "Epoch 00048 | Time(s) 0.2803 | Loss(train) 0.2531 | Acc(train) 0.9214 | Loss(val) 0.7850 | Acc(val) 0.7680 | \n",
      "Epoch 00049 | Time(s) 0.2835 | Loss(train) 0.2471 | Acc(train) 0.9214 | Loss(val) 0.7798 | Acc(val) 0.7700 | \n",
      "Epoch 00050 | Time(s) 0.2823 | Loss(train) 0.2509 | Acc(train) 0.9143 | Loss(val) 0.7668 | Acc(val) 0.7800 | \n",
      "Epoch 00051 | Time(s) 0.2673 | Loss(train) 0.2471 | Acc(train) 0.9214 | Loss(val) 0.7575 | Acc(val) 0.7780 | \n",
      "Epoch 00052 | Time(s) 0.3116 | Loss(train) 0.2355 | Acc(train) 0.9286 | Loss(val) 0.7618 | Acc(val) 0.7800 | \n",
      "Epoch 00053 | Time(s) 0.2749 | Loss(train) 0.2338 | Acc(train) 0.9357 | Loss(val) 0.7744 | Acc(val) 0.7740 | \n",
      "Epoch 00054 | Time(s) 0.2752 | Loss(train) 0.2353 | Acc(train) 0.9071 | Loss(val) 0.7879 | Acc(val) 0.7760 | \n",
      "Epoch 00055 | Time(s) 0.2785 | Loss(train) 0.2291 | Acc(train) 0.9286 | Loss(val) 0.7994 | Acc(val) 0.7740 | \n",
      "Epoch 00056 | Time(s) 0.2794 | Loss(train) 0.2234 | Acc(train) 0.9286 | Loss(val) 0.8102 | Acc(val) 0.7720 | \n",
      "Epoch 00057 | Time(s) 0.2800 | Loss(train) 0.2289 | Acc(train) 0.9143 | Loss(val) 0.8056 | Acc(val) 0.7760 | \n",
      "Epoch 00058 | Time(s) 0.2888 | Loss(train) 0.2195 | Acc(train) 0.9286 | Loss(val) 0.7911 | Acc(val) 0.7760 | \n",
      "Epoch 00059 | Time(s) 0.2801 | Loss(train) 0.2242 | Acc(train) 0.9214 | Loss(val) 0.7852 | Acc(val) 0.7760 | \n",
      "Epoch 00060 | Time(s) 0.2984 | Loss(train) 0.2123 | Acc(train) 0.9143 | Loss(val) 0.7908 | Acc(val) 0.7740 | \n",
      "Epoch 00061 | Time(s) 0.2920 | Loss(train) 0.2119 | Acc(train) 0.9143 | Loss(val) 0.8063 | Acc(val) 0.7720 | \n",
      "Epoch 00062 | Time(s) 0.3083 | Loss(train) 0.2061 | Acc(train) 0.9286 | Loss(val) 0.8255 | Acc(val) 0.7700 | \n",
      "Epoch 00063 | Time(s) 0.2687 | Loss(train) 0.2072 | Acc(train) 0.9286 | Loss(val) 0.8330 | Acc(val) 0.7660 | \n",
      "Epoch 00064 | Time(s) 0.2822 | Loss(train) 0.1969 | Acc(train) 0.9357 | Loss(val) 0.8362 | Acc(val) 0.7720 | \n",
      "Epoch 00065 | Time(s) 0.3143 | Loss(train) 0.1981 | Acc(train) 0.9357 | Loss(val) 0.8311 | Acc(val) 0.7720 | \n",
      "Epoch 00066 | Time(s) 0.2669 | Loss(train) 0.1900 | Acc(train) 0.9429 | Loss(val) 0.8290 | Acc(val) 0.7780 | \n",
      "Epoch 00067 | Time(s) 0.2944 | Loss(train) 0.1805 | Acc(train) 0.9500 | Loss(val) 0.8346 | Acc(val) 0.7740 | \n",
      "Epoch 00068 | Time(s) 0.2769 | Loss(train) 0.1803 | Acc(train) 0.9429 | Loss(val) 0.8477 | Acc(val) 0.7740 | \n",
      "Epoch 00069 | Time(s) 0.2801 | Loss(train) 0.1755 | Acc(train) 0.9500 | Loss(val) 0.8588 | Acc(val) 0.7760 | \n",
      "Epoch 00070 | Time(s) 0.2826 | Loss(train) 0.1640 | Acc(train) 0.9714 | Loss(val) 0.8686 | Acc(val) 0.7740 | \n",
      "Epoch 00071 | Time(s) 0.3092 | Loss(train) 0.1629 | Acc(train) 0.9643 | Loss(val) 0.8663 | Acc(val) 0.7800 | \n",
      "Epoch 00072 | Time(s) 0.2941 | Loss(train) 0.1591 | Acc(train) 0.9571 | Loss(val) 0.8605 | Acc(val) 0.7720 | \n",
      "Epoch 00073 | Time(s) 0.2782 | Loss(train) 0.1559 | Acc(train) 0.9571 | Loss(val) 0.8611 | Acc(val) 0.7680 | \n",
      "Epoch 00074 | Time(s) 0.3023 | Loss(train) 0.1587 | Acc(train) 0.9500 | Loss(val) 0.8661 | Acc(val) 0.7660 | \n",
      "Epoch 00075 | Time(s) 0.2865 | Loss(train) 0.1470 | Acc(train) 0.9571 | Loss(val) 0.8758 | Acc(val) 0.7640 | \n",
      "Epoch 00076 | Time(s) 0.2782 | Loss(train) 0.1428 | Acc(train) 0.9571 | Loss(val) 0.8846 | Acc(val) 0.7620 | \n",
      "Epoch 00077 | Time(s) 0.2868 | Loss(train) 0.1412 | Acc(train) 0.9643 | Loss(val) 0.8821 | Acc(val) 0.7620 | \n",
      "Epoch 00078 | Time(s) 0.2796 | Loss(train) 0.1365 | Acc(train) 0.9786 | Loss(val) 0.8862 | Acc(val) 0.7560 | \n",
      "Epoch 00079 | Time(s) 0.2678 | Loss(train) 0.1303 | Acc(train) 0.9786 | Loss(val) 0.8915 | Acc(val) 0.7580 | \n",
      "Epoch 00080 | Time(s) 0.2921 | Loss(train) 0.1311 | Acc(train) 0.9714 | Loss(val) 0.9022 | Acc(val) 0.7520 | \n",
      "Epoch 00081 | Time(s) 0.2654 | Loss(train) 0.1198 | Acc(train) 0.9786 | Loss(val) 0.9082 | Acc(val) 0.7620 | \n",
      "Epoch 00082 | Time(s) 0.2961 | Loss(train) 0.1224 | Acc(train) 0.9786 | Loss(val) 0.9158 | Acc(val) 0.7600 | \n",
      "Epoch 00083 | Time(s) 0.2833 | Loss(train) 0.1155 | Acc(train) 0.9786 | Loss(val) 0.9173 | Acc(val) 0.7580 | \n",
      "Epoch 00084 | Time(s) 0.2665 | Loss(train) 0.1128 | Acc(train) 0.9786 | Loss(val) 0.9156 | Acc(val) 0.7540 | \n",
      "Epoch 00085 | Time(s) 0.2844 | Loss(train) 0.1124 | Acc(train) 0.9786 | Loss(val) 0.9141 | Acc(val) 0.7580 | \n",
      "Epoch 00086 | Time(s) 0.2783 | Loss(train) 0.1067 | Acc(train) 0.9786 | Loss(val) 0.9237 | Acc(val) 0.7640 | \n",
      "Epoch 00087 | Time(s) 0.2826 | Loss(train) 0.1009 | Acc(train) 0.9786 | Loss(val) 0.9458 | Acc(val) 0.7640 | \n",
      "Epoch 00088 | Time(s) 0.2982 | Loss(train) 0.1005 | Acc(train) 0.9857 | Loss(val) 0.9686 | Acc(val) 0.7640 | \n",
      "Epoch 00089 | Time(s) 0.2829 | Loss(train) 0.1041 | Acc(train) 0.9857 | Loss(val) 0.9881 | Acc(val) 0.7640 | \n",
      "Epoch 00090 | Time(s) 0.2819 | Loss(train) 0.1001 | Acc(train) 0.9786 | Loss(val) 0.9937 | Acc(val) 0.7600 | \n",
      "Epoch 00091 | Time(s) 0.2863 | Loss(train) 0.0980 | Acc(train) 0.9786 | Loss(val) 0.9851 | Acc(val) 0.7600 | \n",
      "Epoch 00092 | Time(s) 0.2825 | Loss(train) 0.0942 | Acc(train) 0.9929 | Loss(val) 0.9737 | Acc(val) 0.7680 | \n",
      "Epoch 00093 | Time(s) 0.2896 | Loss(train) 0.0929 | Acc(train) 0.9786 | Loss(val) 0.9782 | Acc(val) 0.7700 | \n",
      "Epoch 00094 | Time(s) 0.2820 | Loss(train) 0.0895 | Acc(train) 0.9857 | Loss(val) 0.9939 | Acc(val) 0.7640 | \n",
      "Epoch 00095 | Time(s) 0.2674 | Loss(train) 0.0870 | Acc(train) 0.9857 | Loss(val) 1.0140 | Acc(val) 0.7600 | \n",
      "Epoch 00096 | Time(s) 0.2815 | Loss(train) 0.0871 | Acc(train) 0.9857 | Loss(val) 1.0367 | Acc(val) 0.7620 | \n",
      "Epoch 00097 | Time(s) 0.2830 | Loss(train) 0.0815 | Acc(train) 0.9857 | Loss(val) 1.0626 | Acc(val) 0.7540 | \n",
      "Epoch 00098 | Time(s) 0.2818 | Loss(train) 0.0868 | Acc(train) 0.9857 | Loss(val) 1.0751 | Acc(val) 0.7500 | \n",
      "Epoch 00099 | Time(s) 0.3036 | Loss(train) 0.0858 | Acc(train) 0.9857 | Loss(val) 1.0693 | Acc(val) 0.7580 | \n",
      "Epoch 00100 | Time(s) 0.2647 | Loss(train) 0.0820 | Acc(train) 0.9857 | Loss(val) 1.0577 | Acc(val) 0.7620 | \n",
      "Optimization Finished!\n",
      "Time(s): 12.5828\n",
      "Loss(test) 0.5644 | Acc(test) 0.8290\n",
      "All runs:\n",
      "Highest Train: 91.43 ± 0.71\n",
      "Highest Valid: 78.87 ± 0.42\n",
      "   Final Test: 82.60 ± 0.44\n"
     ]
    },
    {
     "data": {
      "text/plain": "(82.59999999999998, 0.43588989435406705)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opengsl.module.solver import GSLSolver\n",
    "from opengsl import ExpManager\n",
    "import argparse\n",
    "\n",
    "class MyGSL(GSLSolver):\n",
    "    def set_method(self):\n",
    "        encoder = GCNDiagEncoder(2, dataset.dim_feats)\n",
    "        metric = Cosine()\n",
    "        postprocess = [KNN(150)]\n",
    "        fuse = Interpolate(1, 1)\n",
    "        # build the graphlearner\n",
    "        self.graphlearner = GraphLearner(encoder=encoder, metric=metric, postprocess=postprocess, fuse=fuse).to(device)\n",
    "        # define gnn model\n",
    "        self.model = GNNEncoder_OpenGSL(dataset.dim_feats, n_hidden=64, n_class=dataset.n_classes, n_layers=2, dropout=0.5).to(device)\n",
    "        self.optim = torch.optim.Adam([{'params': self.model.parameters()}, {'params': self.graphlearner.parameters()}], lr=self.conf.training['lr'], weight_decay=self.conf.training['weight_decay'])\n",
    "\n",
    "conf = {'use_deterministic': False,\n",
    "    'model': {'n_hidden': 64, 'n_layer': 2},\n",
    "    'training': {'lr': 1e-2,\n",
    "    'weight_decay': 5e-4,\n",
    "    'n_epochs': 100,\n",
    "    'patience': None,\n",
    "    'criterion': 'metric'},\n",
    "    'dataset': {'feat_norm': False, 'sparse': True},\n",
    "    'analysis': {'flag': False, 'save_graph': False}}\n",
    "mygsl = MyGSL(argparse.Namespace(**conf), dataset)\n",
    "exp = ExpManager(solver=mygsl)\n",
    "exp.run(n_runs=3, debug=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you have learned how to build a simple GSL using our provided components in OpenGSL. Try other components and datasets freely~"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
