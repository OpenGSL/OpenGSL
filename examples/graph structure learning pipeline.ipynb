{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build Your Own Graph Structure Learning Algorithm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook let's try to implement a simple GSL algorithm using OpenGSL.\n",
    "\n",
    "We provide multiple choices for each component in OpenGSL.\n",
    "\n",
    "All of them can be freely chosen and assembled into a **GraphLearner** as shown below.\n",
    "\n",
    "<img src=\"../docs/source/img/package.jpg\" alt=\"描述\" width=\"900\">\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data\n",
    "\n",
    "In this notebook we simply run experiments on *cora*, you can try other datasets on your own."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Data statistics------'\n",
      "                #Nodes 2708\n",
      "                #Edges 5278\n",
      "                #Classes 7\n",
      "----Split statistics of 1 splits------'\n",
      "                #Train samples 140\n",
      "                #Val samples 500\n",
      "                #Test samples 1000\n"
     ]
    }
   ],
   "source": [
    "from opengsl.data.dataset import Dataset\n",
    "dataset = Dataset(\"cora\", n_splits=1)\n",
    "train_mask = dataset.train_masks[0]\n",
    "val_mask = dataset.val_masks[0]\n",
    "test_mask = dataset.test_masks[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build Model\n",
    "\n",
    "It's easy to implement a simple GSL algorithm using our provided components.\n",
    "\n",
    "Let's **choose the basic components and build the graphlearner**.\n",
    "\n",
    "We use the **GCNDiagEncoder** as in *[\"Graph-Revised Convolutional Network\"](https://arxiv.org/abs/1911.07123)*, followed by **Cosine** (metric), **KNN** (transform), **Interpolate** (fuse). Then a **GraphLearner** is built with these components."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "from opengsl.module.encoder import GCNEncoder, GCNDiagEncoder\n",
    "from opengsl.module import GraphLearner\n",
    "from opengsl.module.transform import KNN\n",
    "from opengsl.module.metric import Cosine\n",
    "from opengsl.module.fuse import Interpolate\n",
    "from opengsl.utils import set_seed\n",
    "\n",
    "device = torch.device('cuda')\n",
    "set_seed(42)\n",
    "encoder = GCNDiagEncoder(2, dataset.dim_feats)\n",
    "metric = Cosine()\n",
    "postprocess = [KNN(150)]\n",
    "fuse = Interpolate(1, 1)\n",
    "# build the graphlearner\n",
    "graphlearner = GraphLearner(encoder=encoder, metric=metric, postprocess=postprocess, fuse=fuse).to(device)\n",
    "# define gnn model\n",
    "gnn = GCNEncoder(dataset.dim_feats, nhid=64, nclass=dataset.n_classes, n_layers=2, dropout=0.5).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To learn a new structure, you just need to input feature and original adj as `new_adj = graphlearner(x ,adj)`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "With the defined graphlearner and gnn, you can use them in a common training and evaluation process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\opengsl\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001 | Time(s) 0.5434 | Loss(train) 1.9515 | Acc(train) 0.1429 | Loss(val) 1.8794 | Acc(val) 0.3740 | *\n",
      "Epoch 00002 | Time(s) 0.1974 | Loss(train) 1.9129 | Acc(train) 0.2357 | Loss(val) 1.8233 | Acc(val) 0.6220 | *\n",
      "Epoch 00003 | Time(s) 0.2000 | Loss(train) 1.8381 | Acc(train) 0.5357 | Loss(val) 1.7645 | Acc(val) 0.7100 | *\n",
      "Epoch 00004 | Time(s) 0.1989 | Loss(train) 1.7641 | Acc(train) 0.7143 | Loss(val) 1.6986 | Acc(val) 0.5900 | \n",
      "Epoch 00005 | Time(s) 0.1990 | Loss(train) 1.6764 | Acc(train) 0.6500 | Loss(val) 1.6161 | Acc(val) 0.5900 | \n",
      "Epoch 00006 | Time(s) 0.1972 | Loss(train) 1.5728 | Acc(train) 0.6571 | Loss(val) 1.5193 | Acc(val) 0.6140 | \n",
      "Epoch 00007 | Time(s) 0.2003 | Loss(train) 1.4543 | Acc(train) 0.6571 | Loss(val) 1.4120 | Acc(val) 0.6520 | \n",
      "Epoch 00008 | Time(s) 0.2011 | Loss(train) 1.3379 | Acc(train) 0.7357 | Loss(val) 1.2974 | Acc(val) 0.6880 | \n",
      "Epoch 00009 | Time(s) 0.1961 | Loss(train) 1.2028 | Acc(train) 0.7643 | Loss(val) 1.1808 | Acc(val) 0.7060 | \n",
      "Epoch 00010 | Time(s) 0.1975 | Loss(train) 1.0688 | Acc(train) 0.7929 | Loss(val) 1.0695 | Acc(val) 0.7320 | *\n",
      "Epoch 00011 | Time(s) 0.1972 | Loss(train) 0.9677 | Acc(train) 0.8071 | Loss(val) 0.9695 | Acc(val) 0.7560 | *\n",
      "Epoch 00012 | Time(s) 0.1985 | Loss(train) 0.8510 | Acc(train) 0.8071 | Loss(val) 0.8846 | Acc(val) 0.7640 | *\n",
      "Epoch 00013 | Time(s) 0.1975 | Loss(train) 0.7777 | Acc(train) 0.8429 | Loss(val) 0.8175 | Acc(val) 0.7680 | *\n",
      "Epoch 00014 | Time(s) 0.1985 | Loss(train) 0.7052 | Acc(train) 0.8500 | Loss(val) 0.7659 | Acc(val) 0.7640 | \n",
      "Epoch 00015 | Time(s) 0.2013 | Loss(train) 0.6503 | Acc(train) 0.8429 | Loss(val) 0.7261 | Acc(val) 0.7700 | *\n",
      "Epoch 00016 | Time(s) 0.1976 | Loss(train) 0.6115 | Acc(train) 0.8429 | Loss(val) 0.7007 | Acc(val) 0.7640 | \n",
      "Epoch 00017 | Time(s) 0.1976 | Loss(train) 0.5638 | Acc(train) 0.8429 | Loss(val) 0.6848 | Acc(val) 0.7660 | \n",
      "Epoch 00018 | Time(s) 0.1961 | Loss(train) 0.5310 | Acc(train) 0.8500 | Loss(val) 0.6783 | Acc(val) 0.7640 | \n",
      "Epoch 00019 | Time(s) 0.1973 | Loss(train) 0.5148 | Acc(train) 0.8500 | Loss(val) 0.6749 | Acc(val) 0.7660 | \n",
      "Epoch 00020 | Time(s) 0.2000 | Loss(train) 0.4830 | Acc(train) 0.8500 | Loss(val) 0.6702 | Acc(val) 0.7700 | \n",
      "Epoch 00021 | Time(s) 0.2000 | Loss(train) 0.4566 | Acc(train) 0.8357 | Loss(val) 0.6730 | Acc(val) 0.7740 | *\n",
      "Epoch 00022 | Time(s) 0.2022 | Loss(train) 0.4418 | Acc(train) 0.8714 | Loss(val) 0.6807 | Acc(val) 0.7800 | *\n",
      "Epoch 00023 | Time(s) 0.2002 | Loss(train) 0.4235 | Acc(train) 0.8714 | Loss(val) 0.6897 | Acc(val) 0.7760 | \n",
      "Epoch 00024 | Time(s) 0.1994 | Loss(train) 0.4076 | Acc(train) 0.8786 | Loss(val) 0.7010 | Acc(val) 0.7660 | \n",
      "Epoch 00025 | Time(s) 0.1976 | Loss(train) 0.3947 | Acc(train) 0.8786 | Loss(val) 0.7051 | Acc(val) 0.7680 | \n",
      "Epoch 00026 | Time(s) 0.2007 | Loss(train) 0.3799 | Acc(train) 0.8857 | Loss(val) 0.7026 | Acc(val) 0.7720 | \n",
      "Epoch 00027 | Time(s) 0.2000 | Loss(train) 0.3623 | Acc(train) 0.8857 | Loss(val) 0.7043 | Acc(val) 0.7760 | \n",
      "Epoch 00028 | Time(s) 0.2007 | Loss(train) 0.3560 | Acc(train) 0.8857 | Loss(val) 0.7030 | Acc(val) 0.7760 | \n",
      "Epoch 00029 | Time(s) 0.1986 | Loss(train) 0.3416 | Acc(train) 0.8857 | Loss(val) 0.7055 | Acc(val) 0.7740 | \n",
      "Epoch 00030 | Time(s) 0.2010 | Loss(train) 0.3292 | Acc(train) 0.9071 | Loss(val) 0.7119 | Acc(val) 0.7760 | \n",
      "Epoch 00031 | Time(s) 0.2007 | Loss(train) 0.3238 | Acc(train) 0.9000 | Loss(val) 0.7143 | Acc(val) 0.7800 | \n",
      "Epoch 00032 | Time(s) 0.1999 | Loss(train) 0.3111 | Acc(train) 0.9000 | Loss(val) 0.7170 | Acc(val) 0.7780 | \n",
      "Epoch 00033 | Time(s) 0.1978 | Loss(train) 0.3030 | Acc(train) 0.9000 | Loss(val) 0.7187 | Acc(val) 0.7840 | *\n",
      "Epoch 00034 | Time(s) 0.1982 | Loss(train) 0.3060 | Acc(train) 0.9071 | Loss(val) 0.7149 | Acc(val) 0.7880 | *\n",
      "Epoch 00035 | Time(s) 0.1980 | Loss(train) 0.2970 | Acc(train) 0.9071 | Loss(val) 0.7112 | Acc(val) 0.7860 | \n",
      "Epoch 00036 | Time(s) 0.1970 | Loss(train) 0.3035 | Acc(train) 0.8857 | Loss(val) 0.7220 | Acc(val) 0.7800 | \n",
      "Epoch 00037 | Time(s) 0.1980 | Loss(train) 0.2887 | Acc(train) 0.9143 | Loss(val) 0.7262 | Acc(val) 0.7800 | \n",
      "Epoch 00038 | Time(s) 0.1981 | Loss(train) 0.2851 | Acc(train) 0.9143 | Loss(val) 0.7302 | Acc(val) 0.7800 | \n",
      "Epoch 00039 | Time(s) 0.1971 | Loss(train) 0.2827 | Acc(train) 0.9143 | Loss(val) 0.7383 | Acc(val) 0.7740 | \n",
      "Epoch 00040 | Time(s) 0.1990 | Loss(train) 0.2738 | Acc(train) 0.9214 | Loss(val) 0.7389 | Acc(val) 0.7820 | \n",
      "Epoch 00041 | Time(s) 0.1995 | Loss(train) 0.2688 | Acc(train) 0.9214 | Loss(val) 0.7342 | Acc(val) 0.7860 | \n",
      "Epoch 00042 | Time(s) 0.1988 | Loss(train) 0.2689 | Acc(train) 0.9214 | Loss(val) 0.7302 | Acc(val) 0.7840 | \n",
      "Epoch 00043 | Time(s) 0.1983 | Loss(train) 0.2605 | Acc(train) 0.9286 | Loss(val) 0.7216 | Acc(val) 0.7840 | \n",
      "Epoch 00044 | Time(s) 0.1990 | Loss(train) 0.2495 | Acc(train) 0.9143 | Loss(val) 0.7165 | Acc(val) 0.7860 | \n",
      "Epoch 00045 | Time(s) 0.1995 | Loss(train) 0.2469 | Acc(train) 0.9071 | Loss(val) 0.7197 | Acc(val) 0.7820 | \n",
      "Epoch 00046 | Time(s) 0.1970 | Loss(train) 0.2500 | Acc(train) 0.9214 | Loss(val) 0.7285 | Acc(val) 0.7840 | \n",
      "Epoch 00047 | Time(s) 0.1982 | Loss(train) 0.2328 | Acc(train) 0.9214 | Loss(val) 0.7370 | Acc(val) 0.7820 | \n",
      "Epoch 00048 | Time(s) 0.2003 | Loss(train) 0.2413 | Acc(train) 0.9286 | Loss(val) 0.7358 | Acc(val) 0.7820 | \n",
      "Epoch 00049 | Time(s) 0.1976 | Loss(train) 0.2300 | Acc(train) 0.9214 | Loss(val) 0.7341 | Acc(val) 0.7840 | \n",
      "Epoch 00050 | Time(s) 0.1970 | Loss(train) 0.2335 | Acc(train) 0.9214 | Loss(val) 0.7458 | Acc(val) 0.7780 | \n",
      "Epoch 00051 | Time(s) 0.1971 | Loss(train) 0.2163 | Acc(train) 0.9286 | Loss(val) 0.7499 | Acc(val) 0.7800 | \n",
      "Epoch 00052 | Time(s) 0.1988 | Loss(train) 0.2240 | Acc(train) 0.9286 | Loss(val) 0.7497 | Acc(val) 0.7820 | \n",
      "Epoch 00053 | Time(s) 0.1967 | Loss(train) 0.2179 | Acc(train) 0.9214 | Loss(val) 0.7578 | Acc(val) 0.7780 | \n",
      "Epoch 00054 | Time(s) 0.1980 | Loss(train) 0.2156 | Acc(train) 0.9214 | Loss(val) 0.7599 | Acc(val) 0.7800 | \n",
      "Epoch 00055 | Time(s) 0.1976 | Loss(train) 0.2145 | Acc(train) 0.9143 | Loss(val) 0.7502 | Acc(val) 0.7800 | \n",
      "Epoch 00056 | Time(s) 0.1971 | Loss(train) 0.2048 | Acc(train) 0.9286 | Loss(val) 0.7575 | Acc(val) 0.7760 | \n",
      "Epoch 00057 | Time(s) 0.1982 | Loss(train) 0.2059 | Acc(train) 0.9357 | Loss(val) 0.7683 | Acc(val) 0.7720 | \n",
      "Epoch 00058 | Time(s) 0.1975 | Loss(train) 0.2055 | Acc(train) 0.9286 | Loss(val) 0.7743 | Acc(val) 0.7740 | \n",
      "Epoch 00059 | Time(s) 0.1996 | Loss(train) 0.1918 | Acc(train) 0.9500 | Loss(val) 0.7912 | Acc(val) 0.7760 | \n",
      "Epoch 00060 | Time(s) 0.1993 | Loss(train) 0.1926 | Acc(train) 0.9357 | Loss(val) 0.8088 | Acc(val) 0.7700 | \n",
      "Epoch 00061 | Time(s) 0.1999 | Loss(train) 0.1919 | Acc(train) 0.9357 | Loss(val) 0.8108 | Acc(val) 0.7720 | \n",
      "Epoch 00062 | Time(s) 0.1976 | Loss(train) 0.1924 | Acc(train) 0.9143 | Loss(val) 0.8037 | Acc(val) 0.7760 | \n",
      "Epoch 00063 | Time(s) 0.1984 | Loss(train) 0.1767 | Acc(train) 0.9357 | Loss(val) 0.8023 | Acc(val) 0.7780 | \n",
      "Epoch 00064 | Time(s) 0.1981 | Loss(train) 0.1787 | Acc(train) 0.9429 | Loss(val) 0.8002 | Acc(val) 0.7800 | \n",
      "Epoch 00065 | Time(s) 0.1971 | Loss(train) 0.1801 | Acc(train) 0.9500 | Loss(val) 0.8128 | Acc(val) 0.7780 | \n",
      "Epoch 00066 | Time(s) 0.1975 | Loss(train) 0.1630 | Acc(train) 0.9643 | Loss(val) 0.8475 | Acc(val) 0.7660 | \n",
      "Epoch 00067 | Time(s) 0.1982 | Loss(train) 0.1616 | Acc(train) 0.9286 | Loss(val) 0.8731 | Acc(val) 0.7680 | \n",
      "Epoch 00068 | Time(s) 0.1986 | Loss(train) 0.1609 | Acc(train) 0.9500 | Loss(val) 0.8694 | Acc(val) 0.7720 | \n",
      "Epoch 00069 | Time(s) 0.2001 | Loss(train) 0.1475 | Acc(train) 0.9643 | Loss(val) 0.8536 | Acc(val) 0.7820 | \n",
      "Epoch 00070 | Time(s) 0.1985 | Loss(train) 0.1416 | Acc(train) 0.9643 | Loss(val) 0.8369 | Acc(val) 0.7840 | \n",
      "Epoch 00071 | Time(s) 0.1975 | Loss(train) 0.1436 | Acc(train) 0.9714 | Loss(val) 0.8249 | Acc(val) 0.7820 | \n",
      "Epoch 00072 | Time(s) 0.1976 | Loss(train) 0.1364 | Acc(train) 0.9714 | Loss(val) 0.8316 | Acc(val) 0.7800 | \n",
      "Epoch 00073 | Time(s) 0.1996 | Loss(train) 0.1341 | Acc(train) 0.9714 | Loss(val) 0.8572 | Acc(val) 0.7680 | \n",
      "Epoch 00074 | Time(s) 0.2000 | Loss(train) 0.1245 | Acc(train) 0.9643 | Loss(val) 0.8878 | Acc(val) 0.7720 | \n",
      "Epoch 00075 | Time(s) 0.2006 | Loss(train) 0.1175 | Acc(train) 0.9714 | Loss(val) 0.9036 | Acc(val) 0.7720 | \n",
      "Epoch 00076 | Time(s) 0.1976 | Loss(train) 0.1209 | Acc(train) 0.9643 | Loss(val) 0.9038 | Acc(val) 0.7660 | \n",
      "Epoch 00077 | Time(s) 0.1992 | Loss(train) 0.1108 | Acc(train) 0.9786 | Loss(val) 0.8997 | Acc(val) 0.7720 | \n",
      "Epoch 00078 | Time(s) 0.1983 | Loss(train) 0.1098 | Acc(train) 0.9786 | Loss(val) 0.8889 | Acc(val) 0.7720 | \n",
      "Epoch 00079 | Time(s) 0.1960 | Loss(train) 0.1056 | Acc(train) 0.9857 | Loss(val) 0.8837 | Acc(val) 0.7700 | \n",
      "Epoch 00080 | Time(s) 0.1988 | Loss(train) 0.1000 | Acc(train) 0.9857 | Loss(val) 0.8856 | Acc(val) 0.7660 | \n",
      "Epoch 00081 | Time(s) 0.1980 | Loss(train) 0.0976 | Acc(train) 0.9857 | Loss(val) 0.8927 | Acc(val) 0.7660 | \n",
      "Epoch 00082 | Time(s) 0.1971 | Loss(train) 0.1021 | Acc(train) 0.9857 | Loss(val) 0.9036 | Acc(val) 0.7680 | \n",
      "Epoch 00083 | Time(s) 0.1984 | Loss(train) 0.0983 | Acc(train) 0.9857 | Loss(val) 0.9126 | Acc(val) 0.7660 | \n",
      "Epoch 00084 | Time(s) 0.1965 | Loss(train) 0.0951 | Acc(train) 0.9786 | Loss(val) 0.9134 | Acc(val) 0.7660 | \n",
      "Epoch 00085 | Time(s) 0.1969 | Loss(train) 0.0929 | Acc(train) 0.9786 | Loss(val) 0.9170 | Acc(val) 0.7680 | \n",
      "Epoch 00086 | Time(s) 0.1971 | Loss(train) 0.0958 | Acc(train) 0.9786 | Loss(val) 0.9203 | Acc(val) 0.7740 | \n",
      "Epoch 00087 | Time(s) 0.1984 | Loss(train) 0.0853 | Acc(train) 0.9929 | Loss(val) 0.9286 | Acc(val) 0.7740 | \n",
      "Epoch 00088 | Time(s) 0.1979 | Loss(train) 0.0894 | Acc(train) 0.9929 | Loss(val) 0.9415 | Acc(val) 0.7700 | \n",
      "Epoch 00089 | Time(s) 0.1991 | Loss(train) 0.0843 | Acc(train) 0.9857 | Loss(val) 0.9571 | Acc(val) 0.7640 | \n",
      "Epoch 00090 | Time(s) 0.1981 | Loss(train) 0.0844 | Acc(train) 0.9929 | Loss(val) 0.9655 | Acc(val) 0.7620 | \n",
      "Epoch 00091 | Time(s) 0.1984 | Loss(train) 0.0807 | Acc(train) 0.9929 | Loss(val) 0.9677 | Acc(val) 0.7660 | \n",
      "Epoch 00092 | Time(s) 0.1999 | Loss(train) 0.0777 | Acc(train) 0.9929 | Loss(val) 0.9632 | Acc(val) 0.7720 | \n",
      "Epoch 00093 | Time(s) 0.1987 | Loss(train) 0.0799 | Acc(train) 0.9857 | Loss(val) 0.9603 | Acc(val) 0.7760 | \n",
      "Epoch 00094 | Time(s) 0.2007 | Loss(train) 0.0747 | Acc(train) 0.9929 | Loss(val) 0.9707 | Acc(val) 0.7740 | \n",
      "Epoch 00095 | Time(s) 0.2014 | Loss(train) 0.0803 | Acc(train) 0.9929 | Loss(val) 0.9938 | Acc(val) 0.7760 | \n",
      "Epoch 00096 | Time(s) 0.2025 | Loss(train) 0.0731 | Acc(train) 0.9929 | Loss(val) 1.0210 | Acc(val) 0.7640 | \n",
      "Epoch 00097 | Time(s) 0.2003 | Loss(train) 0.0750 | Acc(train) 0.9929 | Loss(val) 1.0363 | Acc(val) 0.7620 | \n",
      "Epoch 00098 | Time(s) 0.1980 | Loss(train) 0.0740 | Acc(train) 0.9929 | Loss(val) 1.0382 | Acc(val) 0.7640 | \n",
      "Epoch 00099 | Time(s) 0.2005 | Loss(train) 0.0681 | Acc(train) 0.9929 | Loss(val) 1.0368 | Acc(val) 0.7760 | \n",
      "Epoch 00100 | Time(s) 0.1986 | Loss(train) 0.0682 | Acc(train) 1.0000 | Loss(val) 1.0279 | Acc(val) 0.7740 | \n",
      "Optimization Finished!\n",
      "Time(s): 7.1249\n",
      "Loss(test) 0.5475 | Acc(test) 0.8280\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from copy import deepcopy\n",
    "from opengsl.module.functional import normalize\n",
    "from opengsl.utils import accuracy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 100\n",
    "lr = 1e-2\n",
    "wd = 5e-4\n",
    "best_valid = 0\n",
    "gsl_weights =None\n",
    "gnn_weights =None\n",
    "start_time = time.time()\n",
    "optim = torch.optim.Adam([{'params': gnn.parameters()}, {'params': graphlearner.parameters()}], lr=lr, weight_decay=wd)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    improve = ''\n",
    "    t0 = time.time()\n",
    "    gnn.train()\n",
    "    graphlearner.train()\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # forward and backward\n",
    "    adj = graphlearner(dataset.feats, normalize(dataset.adj))\n",
    "    output = gnn(dataset.feats, normalize(adj, add_loop=False))\n",
    "\n",
    "    loss_train = F.cross_entropy(output[train_mask], dataset.labels[train_mask])\n",
    "    acc_train = accuracy(dataset.labels[train_mask].cpu().numpy(), output[train_mask].detach().cpu().numpy())\n",
    "    loss_train.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # Evaluate\n",
    "    gnn.eval()\n",
    "    graphlearner.eval()\n",
    "    with torch.no_grad():\n",
    "        adj = graphlearner(dataset.feats, normalize(dataset.adj))\n",
    "        output = gnn(dataset.feats, normalize(adj, add_loop=False))\n",
    "        loss_val = F.cross_entropy(output[val_mask], dataset.labels[val_mask])\n",
    "        acc_val = accuracy(dataset.labels[val_mask].cpu().numpy(), output[val_mask].detach().cpu().numpy())\n",
    "\n",
    "    # save\n",
    "    if acc_val > best_valid:\n",
    "        improve = '*'\n",
    "        gsl_weights = deepcopy(graphlearner.state_dict())\n",
    "        gnn_weights = deepcopy(gnn.state_dict())\n",
    "        total_time = time.time() - start_time\n",
    "        best_val_loss = loss_val\n",
    "        best_valid = acc_val\n",
    "        best_adj = adj.detach().clone()\n",
    "\n",
    "    # debug\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss(train) {:.4f} | Acc(train) {:.4f} | Loss(val) {:.4f} | Acc(val) {:.4f} | {}\".format(\n",
    "        epoch+1, time.time() -t0, loss_train.item(), acc_train, loss_val, acc_val, improve))\n",
    "\n",
    "print('Optimization Finished!')\n",
    "print('Time(s): {:.4f}'.format(total_time))\n",
    "# test\n",
    "graphlearner.load_state_dict(gsl_weights)\n",
    "gnn.load_state_dict(gnn_weights)\n",
    "with torch.no_grad():\n",
    "    adj = graphlearner(dataset.feats, normalize(dataset.adj))\n",
    "    output = gnn(dataset.feats, normalize(adj, add_loop=False))\n",
    "    loss_test = F.cross_entropy(output[test_mask], dataset.labels[test_mask])\n",
    "    acc_test = accuracy(dataset.labels[test_mask].cpu().numpy(), output[test_mask].detach().cpu().numpy())\n",
    "\n",
    "print(\"Loss(test) {:.4f} | Acc(test) {:.4f}\".format(loss_test.item(), acc_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It seems that Our created GSL algorithm help improve the performance of GCN to 82.8, which is better than the usual 81+."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training and Evaluation using our pipeline\n",
    "We recommend you to use our provided pipline with **GSLSolver** and **ExpManager** to simplify the above process. Only *set_method* needs to be customized in this pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 0/3\n",
      "Epoch 00001 | Time(s) 0.4687 | Loss(train) 1.9483 | Acc(train) 0.1429 | Loss(val) 1.9186 | Acc(val) 0.3100 | *\n",
      "Epoch 00002 | Time(s) 0.2022 | Loss(train) 1.9172 | Acc(train) 0.3714 | Loss(val) 1.8581 | Acc(val) 0.4660 | *\n",
      "Epoch 00003 | Time(s) 0.2005 | Loss(train) 1.8505 | Acc(train) 0.5071 | Loss(val) 1.7884 | Acc(val) 0.5720 | *\n",
      "Epoch 00004 | Time(s) 0.2004 | Loss(train) 1.7720 | Acc(train) 0.6571 | Loss(val) 1.7089 | Acc(val) 0.5860 | *\n",
      "Epoch 00005 | Time(s) 0.2036 | Loss(train) 1.6840 | Acc(train) 0.6643 | Loss(val) 1.6176 | Acc(val) 0.6060 | *\n",
      "Epoch 00006 | Time(s) 0.2013 | Loss(train) 1.5817 | Acc(train) 0.6571 | Loss(val) 1.5162 | Acc(val) 0.6440 | *\n",
      "Epoch 00007 | Time(s) 0.2004 | Loss(train) 1.4627 | Acc(train) 0.6857 | Loss(val) 1.4058 | Acc(val) 0.6720 | *\n",
      "Epoch 00008 | Time(s) 0.2011 | Loss(train) 1.3437 | Acc(train) 0.7000 | Loss(val) 1.2912 | Acc(val) 0.7060 | *\n",
      "Epoch 00009 | Time(s) 0.2026 | Loss(train) 1.1950 | Acc(train) 0.7429 | Loss(val) 1.1743 | Acc(val) 0.7300 | *\n",
      "Epoch 00010 | Time(s) 0.2022 | Loss(train) 1.0698 | Acc(train) 0.8000 | Loss(val) 1.0624 | Acc(val) 0.7320 | *\n",
      "Epoch 00011 | Time(s) 0.2006 | Loss(train) 0.9588 | Acc(train) 0.8214 | Loss(val) 0.9640 | Acc(val) 0.7480 | *\n",
      "Epoch 00012 | Time(s) 0.2014 | Loss(train) 0.8489 | Acc(train) 0.8357 | Loss(val) 0.8808 | Acc(val) 0.7560 | *\n",
      "Epoch 00013 | Time(s) 0.2026 | Loss(train) 0.7613 | Acc(train) 0.8429 | Loss(val) 0.8154 | Acc(val) 0.7620 | *\n",
      "Epoch 00014 | Time(s) 0.2011 | Loss(train) 0.6907 | Acc(train) 0.8286 | Loss(val) 0.7678 | Acc(val) 0.7540 | \n",
      "Epoch 00015 | Time(s) 0.2002 | Loss(train) 0.6316 | Acc(train) 0.8286 | Loss(val) 0.7346 | Acc(val) 0.7600 | \n",
      "Epoch 00016 | Time(s) 0.1994 | Loss(train) 0.5888 | Acc(train) 0.8357 | Loss(val) 0.7101 | Acc(val) 0.7680 | *\n",
      "Epoch 00017 | Time(s) 0.1991 | Loss(train) 0.5456 | Acc(train) 0.8357 | Loss(val) 0.6974 | Acc(val) 0.7700 | *\n",
      "Epoch 00018 | Time(s) 0.2001 | Loss(train) 0.5190 | Acc(train) 0.8286 | Loss(val) 0.6894 | Acc(val) 0.7680 | \n",
      "Epoch 00019 | Time(s) 0.2001 | Loss(train) 0.5057 | Acc(train) 0.8500 | Loss(val) 0.6863 | Acc(val) 0.7660 | \n",
      "Epoch 00020 | Time(s) 0.2022 | Loss(train) 0.4741 | Acc(train) 0.8429 | Loss(val) 0.6820 | Acc(val) 0.7660 | \n",
      "Epoch 00021 | Time(s) 0.2017 | Loss(train) 0.4579 | Acc(train) 0.8357 | Loss(val) 0.6885 | Acc(val) 0.7700 | \n",
      "Epoch 00022 | Time(s) 0.2027 | Loss(train) 0.4381 | Acc(train) 0.8500 | Loss(val) 0.6897 | Acc(val) 0.7680 | \n",
      "Epoch 00023 | Time(s) 0.2009 | Loss(train) 0.4279 | Acc(train) 0.8429 | Loss(val) 0.6925 | Acc(val) 0.7680 | \n",
      "Epoch 00024 | Time(s) 0.2013 | Loss(train) 0.4049 | Acc(train) 0.8643 | Loss(val) 0.6879 | Acc(val) 0.7760 | *\n",
      "Epoch 00025 | Time(s) 0.1997 | Loss(train) 0.3879 | Acc(train) 0.8857 | Loss(val) 0.6798 | Acc(val) 0.7920 | *\n",
      "Epoch 00026 | Time(s) 0.2038 | Loss(train) 0.3791 | Acc(train) 0.9071 | Loss(val) 0.6857 | Acc(val) 0.7860 | \n",
      "Epoch 00027 | Time(s) 0.1993 | Loss(train) 0.3631 | Acc(train) 0.8929 | Loss(val) 0.7034 | Acc(val) 0.7820 | \n",
      "Epoch 00028 | Time(s) 0.2173 | Loss(train) 0.3545 | Acc(train) 0.8714 | Loss(val) 0.7253 | Acc(val) 0.7760 | \n",
      "Epoch 00029 | Time(s) 0.2025 | Loss(train) 0.3400 | Acc(train) 0.8929 | Loss(val) 0.7365 | Acc(val) 0.7720 | \n",
      "Epoch 00030 | Time(s) 0.2014 | Loss(train) 0.3382 | Acc(train) 0.9071 | Loss(val) 0.7298 | Acc(val) 0.7780 | \n",
      "Epoch 00031 | Time(s) 0.2011 | Loss(train) 0.3328 | Acc(train) 0.9071 | Loss(val) 0.7119 | Acc(val) 0.7820 | \n",
      "Epoch 00032 | Time(s) 0.2010 | Loss(train) 0.3291 | Acc(train) 0.8929 | Loss(val) 0.7037 | Acc(val) 0.7840 | \n",
      "Epoch 00033 | Time(s) 0.2045 | Loss(train) 0.3258 | Acc(train) 0.8929 | Loss(val) 0.7108 | Acc(val) 0.7820 | \n",
      "Epoch 00034 | Time(s) 0.2032 | Loss(train) 0.3160 | Acc(train) 0.8857 | Loss(val) 0.7203 | Acc(val) 0.7800 | \n",
      "Epoch 00035 | Time(s) 0.2030 | Loss(train) 0.3023 | Acc(train) 0.8929 | Loss(val) 0.7399 | Acc(val) 0.7700 | \n",
      "Epoch 00036 | Time(s) 0.2031 | Loss(train) 0.2967 | Acc(train) 0.9000 | Loss(val) 0.7425 | Acc(val) 0.7700 | \n",
      "Epoch 00037 | Time(s) 0.2000 | Loss(train) 0.2877 | Acc(train) 0.9071 | Loss(val) 0.7197 | Acc(val) 0.7780 | \n",
      "Epoch 00038 | Time(s) 0.2017 | Loss(train) 0.2745 | Acc(train) 0.9214 | Loss(val) 0.7028 | Acc(val) 0.7800 | \n",
      "Epoch 00039 | Time(s) 0.2018 | Loss(train) 0.2725 | Acc(train) 0.9143 | Loss(val) 0.7026 | Acc(val) 0.7840 | \n",
      "Epoch 00040 | Time(s) 0.2009 | Loss(train) 0.2766 | Acc(train) 0.9071 | Loss(val) 0.7123 | Acc(val) 0.7840 | \n",
      "Epoch 00041 | Time(s) 0.2040 | Loss(train) 0.2736 | Acc(train) 0.9143 | Loss(val) 0.7200 | Acc(val) 0.7820 | \n",
      "Epoch 00042 | Time(s) 0.2034 | Loss(train) 0.2609 | Acc(train) 0.9000 | Loss(val) 0.7344 | Acc(val) 0.7860 | \n",
      "Epoch 00043 | Time(s) 0.2020 | Loss(train) 0.2538 | Acc(train) 0.9143 | Loss(val) 0.7451 | Acc(val) 0.7800 | \n",
      "Epoch 00044 | Time(s) 0.2014 | Loss(train) 0.2506 | Acc(train) 0.9143 | Loss(val) 0.7399 | Acc(val) 0.7820 | \n",
      "Epoch 00045 | Time(s) 0.2013 | Loss(train) 0.2528 | Acc(train) 0.9214 | Loss(val) 0.7170 | Acc(val) 0.7880 | \n",
      "Epoch 00046 | Time(s) 0.1996 | Loss(train) 0.2491 | Acc(train) 0.9214 | Loss(val) 0.7177 | Acc(val) 0.7860 | \n",
      "Epoch 00047 | Time(s) 0.2020 | Loss(train) 0.2389 | Acc(train) 0.9214 | Loss(val) 0.7251 | Acc(val) 0.7820 | \n",
      "Epoch 00048 | Time(s) 0.2035 | Loss(train) 0.2483 | Acc(train) 0.9286 | Loss(val) 0.7450 | Acc(val) 0.7860 | \n",
      "Epoch 00049 | Time(s) 0.2016 | Loss(train) 0.2286 | Acc(train) 0.9286 | Loss(val) 0.7716 | Acc(val) 0.7780 | \n",
      "Epoch 00050 | Time(s) 0.2022 | Loss(train) 0.2291 | Acc(train) 0.9214 | Loss(val) 0.7672 | Acc(val) 0.7780 | \n",
      "Epoch 00051 | Time(s) 0.2041 | Loss(train) 0.2308 | Acc(train) 0.9214 | Loss(val) 0.7412 | Acc(val) 0.7820 | \n",
      "Epoch 00052 | Time(s) 0.2022 | Loss(train) 0.2229 | Acc(train) 0.9071 | Loss(val) 0.7279 | Acc(val) 0.7900 | \n",
      "Epoch 00053 | Time(s) 0.2031 | Loss(train) 0.2325 | Acc(train) 0.9071 | Loss(val) 0.7242 | Acc(val) 0.7860 | \n",
      "Epoch 00054 | Time(s) 0.2040 | Loss(train) 0.2215 | Acc(train) 0.9429 | Loss(val) 0.7434 | Acc(val) 0.7800 | \n",
      "Epoch 00055 | Time(s) 0.2005 | Loss(train) 0.2241 | Acc(train) 0.9214 | Loss(val) 0.7699 | Acc(val) 0.7840 | \n",
      "Epoch 00056 | Time(s) 0.2016 | Loss(train) 0.2269 | Acc(train) 0.9214 | Loss(val) 0.7812 | Acc(val) 0.7860 | \n",
      "Epoch 00057 | Time(s) 0.2012 | Loss(train) 0.2219 | Acc(train) 0.9286 | Loss(val) 0.7715 | Acc(val) 0.7820 | \n",
      "Epoch 00058 | Time(s) 0.2001 | Loss(train) 0.2116 | Acc(train) 0.9214 | Loss(val) 0.7633 | Acc(val) 0.7820 | \n",
      "Epoch 00059 | Time(s) 0.2007 | Loss(train) 0.2172 | Acc(train) 0.9143 | Loss(val) 0.7595 | Acc(val) 0.7840 | \n",
      "Epoch 00060 | Time(s) 0.2021 | Loss(train) 0.2010 | Acc(train) 0.9429 | Loss(val) 0.7663 | Acc(val) 0.7840 | \n",
      "Epoch 00061 | Time(s) 0.2029 | Loss(train) 0.2034 | Acc(train) 0.9429 | Loss(val) 0.7763 | Acc(val) 0.7780 | \n",
      "Epoch 00062 | Time(s) 0.2031 | Loss(train) 0.2184 | Acc(train) 0.9214 | Loss(val) 0.7843 | Acc(val) 0.7800 | \n",
      "Epoch 00063 | Time(s) 0.2013 | Loss(train) 0.1999 | Acc(train) 0.9286 | Loss(val) 0.8021 | Acc(val) 0.7740 | \n",
      "Epoch 00064 | Time(s) 0.2022 | Loss(train) 0.1918 | Acc(train) 0.9357 | Loss(val) 0.8193 | Acc(val) 0.7660 | \n",
      "Epoch 00065 | Time(s) 0.1998 | Loss(train) 0.1957 | Acc(train) 0.9357 | Loss(val) 0.8228 | Acc(val) 0.7700 | \n",
      "Epoch 00066 | Time(s) 0.2008 | Loss(train) 0.1821 | Acc(train) 0.9429 | Loss(val) 0.8193 | Acc(val) 0.7700 | \n",
      "Epoch 00067 | Time(s) 0.2011 | Loss(train) 0.1766 | Acc(train) 0.9500 | Loss(val) 0.8215 | Acc(val) 0.7720 | \n",
      "Epoch 00068 | Time(s) 0.2030 | Loss(train) 0.1703 | Acc(train) 0.9429 | Loss(val) 0.8208 | Acc(val) 0.7640 | \n",
      "Epoch 00069 | Time(s) 0.1998 | Loss(train) 0.1759 | Acc(train) 0.9357 | Loss(val) 0.8257 | Acc(val) 0.7700 | \n",
      "Epoch 00070 | Time(s) 0.2003 | Loss(train) 0.1685 | Acc(train) 0.9429 | Loss(val) 0.8328 | Acc(val) 0.7700 | \n",
      "Epoch 00071 | Time(s) 0.2004 | Loss(train) 0.1583 | Acc(train) 0.9500 | Loss(val) 0.8440 | Acc(val) 0.7660 | \n",
      "Epoch 00072 | Time(s) 0.2036 | Loss(train) 0.1479 | Acc(train) 0.9571 | Loss(val) 0.8588 | Acc(val) 0.7640 | \n",
      "Epoch 00073 | Time(s) 0.2026 | Loss(train) 0.1491 | Acc(train) 0.9500 | Loss(val) 0.8666 | Acc(val) 0.7620 | \n",
      "Epoch 00074 | Time(s) 0.2021 | Loss(train) 0.1438 | Acc(train) 0.9500 | Loss(val) 0.8637 | Acc(val) 0.7580 | \n",
      "Epoch 00075 | Time(s) 0.2039 | Loss(train) 0.1408 | Acc(train) 0.9571 | Loss(val) 0.8580 | Acc(val) 0.7560 | \n",
      "Epoch 00076 | Time(s) 0.2023 | Loss(train) 0.1376 | Acc(train) 0.9714 | Loss(val) 0.8524 | Acc(val) 0.7600 | \n",
      "Epoch 00077 | Time(s) 0.2028 | Loss(train) 0.1326 | Acc(train) 0.9714 | Loss(val) 0.8589 | Acc(val) 0.7660 | \n",
      "Epoch 00078 | Time(s) 0.2082 | Loss(train) 0.1241 | Acc(train) 0.9786 | Loss(val) 0.8703 | Acc(val) 0.7640 | \n",
      "Epoch 00079 | Time(s) 0.2095 | Loss(train) 0.1211 | Acc(train) 0.9786 | Loss(val) 0.8829 | Acc(val) 0.7580 | \n",
      "Epoch 00080 | Time(s) 0.2077 | Loss(train) 0.1161 | Acc(train) 0.9786 | Loss(val) 0.8976 | Acc(val) 0.7520 | \n",
      "Epoch 00081 | Time(s) 0.2084 | Loss(train) 0.1124 | Acc(train) 0.9857 | Loss(val) 0.9094 | Acc(val) 0.7440 | \n",
      "Epoch 00082 | Time(s) 0.2093 | Loss(train) 0.1141 | Acc(train) 0.9786 | Loss(val) 0.9052 | Acc(val) 0.7460 | \n",
      "Epoch 00083 | Time(s) 0.2087 | Loss(train) 0.1094 | Acc(train) 0.9786 | Loss(val) 0.8951 | Acc(val) 0.7520 | \n",
      "Epoch 00084 | Time(s) 0.2091 | Loss(train) 0.1096 | Acc(train) 0.9857 | Loss(val) 0.9006 | Acc(val) 0.7580 | \n",
      "Epoch 00085 | Time(s) 0.2088 | Loss(train) 0.1099 | Acc(train) 0.9786 | Loss(val) 0.9032 | Acc(val) 0.7600 | \n",
      "Epoch 00086 | Time(s) 0.2088 | Loss(train) 0.1005 | Acc(train) 0.9786 | Loss(val) 0.9223 | Acc(val) 0.7580 | \n",
      "Epoch 00087 | Time(s) 0.2071 | Loss(train) 0.0948 | Acc(train) 0.9857 | Loss(val) 0.9427 | Acc(val) 0.7540 | \n",
      "Epoch 00088 | Time(s) 0.2111 | Loss(train) 0.0966 | Acc(train) 0.9857 | Loss(val) 0.9602 | Acc(val) 0.7560 | \n",
      "Epoch 00089 | Time(s) 0.2056 | Loss(train) 0.0925 | Acc(train) 0.9857 | Loss(val) 0.9710 | Acc(val) 0.7600 | \n",
      "Epoch 00090 | Time(s) 0.2036 | Loss(train) 0.0885 | Acc(train) 0.9929 | Loss(val) 0.9806 | Acc(val) 0.7680 | \n",
      "Epoch 00091 | Time(s) 0.2024 | Loss(train) 0.0955 | Acc(train) 0.9786 | Loss(val) 0.9835 | Acc(val) 0.7660 | \n",
      "Epoch 00092 | Time(s) 0.2057 | Loss(train) 0.0826 | Acc(train) 0.9929 | Loss(val) 0.9916 | Acc(val) 0.7600 | \n",
      "Epoch 00093 | Time(s) 0.2056 | Loss(train) 0.0840 | Acc(train) 0.9857 | Loss(val) 0.9866 | Acc(val) 0.7580 | \n",
      "Epoch 00094 | Time(s) 0.2049 | Loss(train) 0.0826 | Acc(train) 0.9857 | Loss(val) 0.9745 | Acc(val) 0.7600 | \n",
      "Epoch 00095 | Time(s) 0.2041 | Loss(train) 0.0794 | Acc(train) 0.9857 | Loss(val) 0.9672 | Acc(val) 0.7620 | \n",
      "Epoch 00096 | Time(s) 0.2051 | Loss(train) 0.0829 | Acc(train) 0.9786 | Loss(val) 0.9848 | Acc(val) 0.7620 | \n",
      "Epoch 00097 | Time(s) 0.2039 | Loss(train) 0.0767 | Acc(train) 0.9857 | Loss(val) 1.0116 | Acc(val) 0.7580 | \n",
      "Epoch 00098 | Time(s) 0.2050 | Loss(train) 0.0719 | Acc(train) 0.9929 | Loss(val) 1.0289 | Acc(val) 0.7580 | \n",
      "Epoch 00099 | Time(s) 0.2151 | Loss(train) 0.0728 | Acc(train) 0.9929 | Loss(val) 1.0376 | Acc(val) 0.7520 | \n",
      "Epoch 00100 | Time(s) 0.2096 | Loss(train) 0.0742 | Acc(train) 0.9857 | Loss(val) 1.0410 | Acc(val) 0.7540 | \n",
      "Optimization Finished!\n",
      "Time(s): 5.3157\n",
      "Loss(test) 0.5442 | Acc(test) 0.8270\n",
      "Exp 1/3\n",
      "Epoch 00001 | Time(s) 0.2084 | Loss(train) 1.9489 | Acc(train) 0.1429 | Loss(val) 1.9200 | Acc(val) 0.2220 | *\n",
      "Epoch 00002 | Time(s) 0.2075 | Loss(train) 1.9118 | Acc(train) 0.3357 | Loss(val) 1.8613 | Acc(val) 0.6460 | *\n",
      "Epoch 00003 | Time(s) 0.2077 | Loss(train) 1.8522 | Acc(train) 0.6714 | Loss(val) 1.7924 | Acc(val) 0.6440 | \n",
      "Epoch 00004 | Time(s) 0.2060 | Loss(train) 1.7832 | Acc(train) 0.6357 | Loss(val) 1.7144 | Acc(val) 0.6700 | *\n",
      "Epoch 00005 | Time(s) 0.2069 | Loss(train) 1.7009 | Acc(train) 0.6500 | Loss(val) 1.6264 | Acc(val) 0.6900 | *\n",
      "Epoch 00006 | Time(s) 0.2070 | Loss(train) 1.6034 | Acc(train) 0.6857 | Loss(val) 1.5289 | Acc(val) 0.7140 | *\n",
      "Epoch 00007 | Time(s) 0.2058 | Loss(train) 1.4973 | Acc(train) 0.7500 | Loss(val) 1.4234 | Acc(val) 0.7440 | *\n",
      "Epoch 00008 | Time(s) 0.2061 | Loss(train) 1.3812 | Acc(train) 0.7929 | Loss(val) 1.3123 | Acc(val) 0.7460 | *\n",
      "Epoch 00009 | Time(s) 0.2071 | Loss(train) 1.2510 | Acc(train) 0.8357 | Loss(val) 1.1999 | Acc(val) 0.7480 | *\n",
      "Epoch 00010 | Time(s) 0.2084 | Loss(train) 1.1302 | Acc(train) 0.8429 | Loss(val) 1.0912 | Acc(val) 0.7540 | *\n",
      "Epoch 00011 | Time(s) 0.2038 | Loss(train) 1.0040 | Acc(train) 0.8429 | Loss(val) 0.9919 | Acc(val) 0.7560 | *\n",
      "Epoch 00012 | Time(s) 0.2056 | Loss(train) 0.8958 | Acc(train) 0.8214 | Loss(val) 0.9077 | Acc(val) 0.7580 | *\n",
      "Epoch 00013 | Time(s) 0.2244 | Loss(train) 0.8032 | Acc(train) 0.8429 | Loss(val) 0.8360 | Acc(val) 0.7600 | *\n",
      "Epoch 00014 | Time(s) 0.2249 | Loss(train) 0.7266 | Acc(train) 0.8143 | Loss(val) 0.7806 | Acc(val) 0.7620 | *\n",
      "Epoch 00015 | Time(s) 0.2174 | Loss(train) 0.6669 | Acc(train) 0.8286 | Loss(val) 0.7431 | Acc(val) 0.7660 | *\n",
      "Epoch 00016 | Time(s) 0.2097 | Loss(train) 0.6187 | Acc(train) 0.8286 | Loss(val) 0.7262 | Acc(val) 0.7640 | \n",
      "Epoch 00017 | Time(s) 0.2108 | Loss(train) 0.5838 | Acc(train) 0.8429 | Loss(val) 0.7154 | Acc(val) 0.7640 | \n",
      "Epoch 00018 | Time(s) 0.2088 | Loss(train) 0.5465 | Acc(train) 0.8500 | Loss(val) 0.7014 | Acc(val) 0.7660 | \n",
      "Epoch 00019 | Time(s) 0.2089 | Loss(train) 0.5219 | Acc(train) 0.8429 | Loss(val) 0.6907 | Acc(val) 0.7680 | *\n",
      "Epoch 00020 | Time(s) 0.2095 | Loss(train) 0.5019 | Acc(train) 0.8429 | Loss(val) 0.6842 | Acc(val) 0.7700 | *\n",
      "Epoch 00021 | Time(s) 0.2077 | Loss(train) 0.4757 | Acc(train) 0.8429 | Loss(val) 0.6827 | Acc(val) 0.7720 | *\n",
      "Epoch 00022 | Time(s) 0.2080 | Loss(train) 0.4586 | Acc(train) 0.8429 | Loss(val) 0.6859 | Acc(val) 0.7740 | *\n",
      "Epoch 00023 | Time(s) 0.2101 | Loss(train) 0.4407 | Acc(train) 0.8714 | Loss(val) 0.6933 | Acc(val) 0.7740 | \n",
      "Epoch 00024 | Time(s) 0.2085 | Loss(train) 0.4275 | Acc(train) 0.8714 | Loss(val) 0.6947 | Acc(val) 0.7740 | \n",
      "Epoch 00025 | Time(s) 0.2096 | Loss(train) 0.4057 | Acc(train) 0.8857 | Loss(val) 0.7000 | Acc(val) 0.7720 | \n",
      "Epoch 00026 | Time(s) 0.2086 | Loss(train) 0.3908 | Acc(train) 0.8643 | Loss(val) 0.7058 | Acc(val) 0.7740 | \n",
      "Epoch 00027 | Time(s) 0.2102 | Loss(train) 0.3755 | Acc(train) 0.8857 | Loss(val) 0.7072 | Acc(val) 0.7760 | *\n",
      "Epoch 00028 | Time(s) 0.2091 | Loss(train) 0.3753 | Acc(train) 0.8929 | Loss(val) 0.7024 | Acc(val) 0.7800 | *\n",
      "Epoch 00029 | Time(s) 0.2079 | Loss(train) 0.3483 | Acc(train) 0.9000 | Loss(val) 0.6943 | Acc(val) 0.7840 | *\n",
      "Epoch 00030 | Time(s) 0.2098 | Loss(train) 0.3450 | Acc(train) 0.8929 | Loss(val) 0.6882 | Acc(val) 0.7780 | \n",
      "Epoch 00031 | Time(s) 0.2096 | Loss(train) 0.3376 | Acc(train) 0.9000 | Loss(val) 0.6810 | Acc(val) 0.7820 | \n",
      "Epoch 00032 | Time(s) 0.2082 | Loss(train) 0.3330 | Acc(train) 0.8929 | Loss(val) 0.6890 | Acc(val) 0.7820 | \n",
      "Epoch 00033 | Time(s) 0.2088 | Loss(train) 0.3198 | Acc(train) 0.8929 | Loss(val) 0.7072 | Acc(val) 0.7820 | \n",
      "Epoch 00034 | Time(s) 0.2088 | Loss(train) 0.3206 | Acc(train) 0.8857 | Loss(val) 0.7236 | Acc(val) 0.7840 | \n",
      "Epoch 00035 | Time(s) 0.2115 | Loss(train) 0.3072 | Acc(train) 0.9071 | Loss(val) 0.7307 | Acc(val) 0.7800 | \n",
      "Epoch 00036 | Time(s) 0.2071 | Loss(train) 0.3028 | Acc(train) 0.9000 | Loss(val) 0.7126 | Acc(val) 0.7840 | \n",
      "Epoch 00037 | Time(s) 0.2115 | Loss(train) 0.2966 | Acc(train) 0.9214 | Loss(val) 0.6931 | Acc(val) 0.7860 | *\n",
      "Epoch 00038 | Time(s) 0.2186 | Loss(train) 0.2853 | Acc(train) 0.9357 | Loss(val) 0.6867 | Acc(val) 0.7880 | *\n",
      "Epoch 00039 | Time(s) 0.2093 | Loss(train) 0.2885 | Acc(train) 0.9071 | Loss(val) 0.6910 | Acc(val) 0.7780 | \n",
      "Epoch 00040 | Time(s) 0.2087 | Loss(train) 0.2890 | Acc(train) 0.9000 | Loss(val) 0.7099 | Acc(val) 0.7800 | \n",
      "Epoch 00041 | Time(s) 0.2090 | Loss(train) 0.2686 | Acc(train) 0.9071 | Loss(val) 0.7272 | Acc(val) 0.7780 | \n",
      "Epoch 00042 | Time(s) 0.2168 | Loss(train) 0.2641 | Acc(train) 0.9286 | Loss(val) 0.7290 | Acc(val) 0.7800 | \n",
      "Epoch 00043 | Time(s) 0.2081 | Loss(train) 0.2618 | Acc(train) 0.9286 | Loss(val) 0.7217 | Acc(val) 0.7820 | \n",
      "Epoch 00044 | Time(s) 0.2133 | Loss(train) 0.2592 | Acc(train) 0.9143 | Loss(val) 0.7191 | Acc(val) 0.7880 | \n",
      "Epoch 00045 | Time(s) 0.2490 | Loss(train) 0.2519 | Acc(train) 0.9214 | Loss(val) 0.7231 | Acc(val) 0.7840 | \n",
      "Epoch 00046 | Time(s) 0.2360 | Loss(train) 0.2447 | Acc(train) 0.9214 | Loss(val) 0.7251 | Acc(val) 0.7860 | \n",
      "Epoch 00047 | Time(s) 0.2176 | Loss(train) 0.2396 | Acc(train) 0.9071 | Loss(val) 0.7296 | Acc(val) 0.7860 | \n",
      "Epoch 00048 | Time(s) 0.2177 | Loss(train) 0.2332 | Acc(train) 0.9143 | Loss(val) 0.7340 | Acc(val) 0.7880 | \n",
      "Epoch 00049 | Time(s) 0.2101 | Loss(train) 0.2303 | Acc(train) 0.9143 | Loss(val) 0.7420 | Acc(val) 0.7860 | \n",
      "Epoch 00050 | Time(s) 0.2057 | Loss(train) 0.2302 | Acc(train) 0.9214 | Loss(val) 0.7492 | Acc(val) 0.7860 | \n",
      "Epoch 00051 | Time(s) 0.2070 | Loss(train) 0.2267 | Acc(train) 0.9357 | Loss(val) 0.7503 | Acc(val) 0.7840 | \n",
      "Epoch 00052 | Time(s) 0.2180 | Loss(train) 0.2336 | Acc(train) 0.9286 | Loss(val) 0.7440 | Acc(val) 0.7840 | \n",
      "Epoch 00053 | Time(s) 0.2078 | Loss(train) 0.2219 | Acc(train) 0.9214 | Loss(val) 0.7469 | Acc(val) 0.7820 | \n",
      "Epoch 00054 | Time(s) 0.2181 | Loss(train) 0.2243 | Acc(train) 0.9214 | Loss(val) 0.7474 | Acc(val) 0.7820 | \n",
      "Epoch 00055 | Time(s) 0.2229 | Loss(train) 0.2216 | Acc(train) 0.9214 | Loss(val) 0.7545 | Acc(val) 0.7840 | \n",
      "Epoch 00056 | Time(s) 0.2101 | Loss(train) 0.2195 | Acc(train) 0.9357 | Loss(val) 0.7658 | Acc(val) 0.7860 | \n",
      "Epoch 00057 | Time(s) 0.2070 | Loss(train) 0.2153 | Acc(train) 0.9357 | Loss(val) 0.7799 | Acc(val) 0.7820 | \n",
      "Epoch 00058 | Time(s) 0.2067 | Loss(train) 0.2073 | Acc(train) 0.9286 | Loss(val) 0.7733 | Acc(val) 0.7860 | \n",
      "Epoch 00059 | Time(s) 0.2405 | Loss(train) 0.2082 | Acc(train) 0.9286 | Loss(val) 0.7700 | Acc(val) 0.7780 | \n",
      "Epoch 00060 | Time(s) 0.2120 | Loss(train) 0.1995 | Acc(train) 0.9214 | Loss(val) 0.7743 | Acc(val) 0.7780 | \n",
      "Epoch 00061 | Time(s) 0.2478 | Loss(train) 0.2028 | Acc(train) 0.9286 | Loss(val) 0.7839 | Acc(val) 0.7740 | \n",
      "Epoch 00062 | Time(s) 0.2460 | Loss(train) 0.1950 | Acc(train) 0.9286 | Loss(val) 0.7946 | Acc(val) 0.7740 | \n",
      "Epoch 00063 | Time(s) 0.2255 | Loss(train) 0.1882 | Acc(train) 0.9357 | Loss(val) 0.8114 | Acc(val) 0.7720 | \n",
      "Epoch 00064 | Time(s) 0.2094 | Loss(train) 0.1815 | Acc(train) 0.9571 | Loss(val) 0.8122 | Acc(val) 0.7660 | \n",
      "Epoch 00065 | Time(s) 0.2078 | Loss(train) 0.1827 | Acc(train) 0.9571 | Loss(val) 0.8039 | Acc(val) 0.7700 | \n",
      "Epoch 00066 | Time(s) 0.2206 | Loss(train) 0.1746 | Acc(train) 0.9500 | Loss(val) 0.8048 | Acc(val) 0.7740 | \n",
      "Epoch 00067 | Time(s) 0.2258 | Loss(train) 0.1698 | Acc(train) 0.9571 | Loss(val) 0.8130 | Acc(val) 0.7700 | \n",
      "Epoch 00068 | Time(s) 0.2262 | Loss(train) 0.1660 | Acc(train) 0.9571 | Loss(val) 0.8229 | Acc(val) 0.7660 | \n",
      "Epoch 00069 | Time(s) 0.2264 | Loss(train) 0.1612 | Acc(train) 0.9571 | Loss(val) 0.8323 | Acc(val) 0.7640 | \n",
      "Epoch 00070 | Time(s) 0.2275 | Loss(train) 0.1611 | Acc(train) 0.9571 | Loss(val) 0.8328 | Acc(val) 0.7660 | \n",
      "Epoch 00071 | Time(s) 0.2259 | Loss(train) 0.1477 | Acc(train) 0.9643 | Loss(val) 0.8291 | Acc(val) 0.7700 | \n",
      "Epoch 00072 | Time(s) 0.2261 | Loss(train) 0.1505 | Acc(train) 0.9500 | Loss(val) 0.8349 | Acc(val) 0.7720 | \n",
      "Epoch 00073 | Time(s) 0.2256 | Loss(train) 0.1427 | Acc(train) 0.9571 | Loss(val) 0.8482 | Acc(val) 0.7660 | \n",
      "Epoch 00074 | Time(s) 0.2266 | Loss(train) 0.1343 | Acc(train) 0.9643 | Loss(val) 0.8510 | Acc(val) 0.7640 | \n",
      "Epoch 00075 | Time(s) 0.2131 | Loss(train) 0.1326 | Acc(train) 0.9643 | Loss(val) 0.8510 | Acc(val) 0.7600 | \n",
      "Epoch 00076 | Time(s) 0.2064 | Loss(train) 0.1263 | Acc(train) 0.9786 | Loss(val) 0.8440 | Acc(val) 0.7700 | \n",
      "Epoch 00077 | Time(s) 0.2067 | Loss(train) 0.1237 | Acc(train) 0.9786 | Loss(val) 0.8462 | Acc(val) 0.7780 | \n",
      "Epoch 00078 | Time(s) 0.2313 | Loss(train) 0.1174 | Acc(train) 0.9714 | Loss(val) 0.8533 | Acc(val) 0.7780 | \n",
      "Epoch 00079 | Time(s) 0.2395 | Loss(train) 0.1144 | Acc(train) 0.9714 | Loss(val) 0.8543 | Acc(val) 0.7740 | \n",
      "Epoch 00080 | Time(s) 0.2147 | Loss(train) 0.1133 | Acc(train) 0.9714 | Loss(val) 0.8522 | Acc(val) 0.7640 | \n",
      "Epoch 00081 | Time(s) 0.2131 | Loss(train) 0.1110 | Acc(train) 0.9714 | Loss(val) 0.8638 | Acc(val) 0.7620 | \n",
      "Epoch 00082 | Time(s) 0.2139 | Loss(train) 0.1075 | Acc(train) 0.9786 | Loss(val) 0.8857 | Acc(val) 0.7640 | \n",
      "Epoch 00083 | Time(s) 0.2105 | Loss(train) 0.1079 | Acc(train) 0.9857 | Loss(val) 0.9061 | Acc(val) 0.7620 | \n",
      "Epoch 00084 | Time(s) 0.2118 | Loss(train) 0.1055 | Acc(train) 0.9857 | Loss(val) 0.9094 | Acc(val) 0.7680 | \n",
      "Epoch 00085 | Time(s) 0.2144 | Loss(train) 0.1014 | Acc(train) 0.9786 | Loss(val) 0.9001 | Acc(val) 0.7660 | \n",
      "Epoch 00086 | Time(s) 0.2206 | Loss(train) 0.1007 | Acc(train) 0.9786 | Loss(val) 0.8864 | Acc(val) 0.7680 | \n",
      "Epoch 00087 | Time(s) 0.2377 | Loss(train) 0.0961 | Acc(train) 0.9786 | Loss(val) 0.8907 | Acc(val) 0.7660 | \n",
      "Epoch 00088 | Time(s) 0.2202 | Loss(train) 0.0937 | Acc(train) 0.9857 | Loss(val) 0.9172 | Acc(val) 0.7620 | \n",
      "Epoch 00089 | Time(s) 0.2104 | Loss(train) 0.0949 | Acc(train) 0.9857 | Loss(val) 0.9415 | Acc(val) 0.7620 | \n",
      "Epoch 00090 | Time(s) 0.2114 | Loss(train) 0.0950 | Acc(train) 0.9857 | Loss(val) 0.9729 | Acc(val) 0.7660 | \n",
      "Epoch 00091 | Time(s) 0.2131 | Loss(train) 0.0928 | Acc(train) 0.9857 | Loss(val) 0.9885 | Acc(val) 0.7540 | \n",
      "Epoch 00092 | Time(s) 0.2127 | Loss(train) 0.0914 | Acc(train) 0.9643 | Loss(val) 0.9866 | Acc(val) 0.7520 | \n",
      "Epoch 00093 | Time(s) 0.2124 | Loss(train) 0.0908 | Acc(train) 0.9786 | Loss(val) 0.9824 | Acc(val) 0.7580 | \n",
      "Epoch 00094 | Time(s) 0.2132 | Loss(train) 0.0875 | Acc(train) 0.9857 | Loss(val) 0.9814 | Acc(val) 0.7580 | \n",
      "Epoch 00095 | Time(s) 0.2140 | Loss(train) 0.0841 | Acc(train) 0.9857 | Loss(val) 0.9868 | Acc(val) 0.7560 | \n",
      "Epoch 00096 | Time(s) 0.2127 | Loss(train) 0.0901 | Acc(train) 0.9786 | Loss(val) 1.0007 | Acc(val) 0.7560 | \n",
      "Epoch 00097 | Time(s) 0.2137 | Loss(train) 0.0781 | Acc(train) 0.9857 | Loss(val) 1.0244 | Acc(val) 0.7600 | \n",
      "Epoch 00098 | Time(s) 0.2121 | Loss(train) 0.0894 | Acc(train) 0.9714 | Loss(val) 1.0382 | Acc(val) 0.7600 | \n",
      "Epoch 00099 | Time(s) 0.2125 | Loss(train) 0.0830 | Acc(train) 0.9714 | Loss(val) 1.0400 | Acc(val) 0.7580 | \n",
      "Epoch 00100 | Time(s) 0.2392 | Loss(train) 0.0798 | Acc(train) 0.9714 | Loss(val) 1.0389 | Acc(val) 0.7600 | \n",
      "Optimization Finished!\n",
      "Time(s): 7.9903\n",
      "Loss(test) 0.5304 | Acc(test) 0.8330\n",
      "Exp 2/3\n",
      "Epoch 00001 | Time(s) 0.2360 | Loss(train) 1.9486 | Acc(train) 0.1429 | Loss(val) 1.9375 | Acc(val) 0.2420 | *\n",
      "Epoch 00002 | Time(s) 0.2211 | Loss(train) 1.9128 | Acc(train) 0.3429 | Loss(val) 1.8928 | Acc(val) 0.3400 | *\n",
      "Epoch 00003 | Time(s) 0.2141 | Loss(train) 1.8523 | Acc(train) 0.4500 | Loss(val) 1.8420 | Acc(val) 0.4280 | *\n",
      "Epoch 00004 | Time(s) 0.2112 | Loss(train) 1.7853 | Acc(train) 0.5357 | Loss(val) 1.7757 | Acc(val) 0.4620 | *\n",
      "Epoch 00005 | Time(s) 0.2112 | Loss(train) 1.7039 | Acc(train) 0.5643 | Loss(val) 1.6937 | Acc(val) 0.4880 | *\n",
      "Epoch 00006 | Time(s) 0.2121 | Loss(train) 1.6094 | Acc(train) 0.6071 | Loss(val) 1.5986 | Acc(val) 0.5740 | *\n",
      "Epoch 00007 | Time(s) 0.2102 | Loss(train) 1.5076 | Acc(train) 0.6429 | Loss(val) 1.4930 | Acc(val) 0.6240 | *\n",
      "Epoch 00008 | Time(s) 0.2151 | Loss(train) 1.3963 | Acc(train) 0.6857 | Loss(val) 1.3833 | Acc(val) 0.6680 | *\n",
      "Epoch 00009 | Time(s) 0.2127 | Loss(train) 1.2785 | Acc(train) 0.6786 | Loss(val) 1.2718 | Acc(val) 0.6860 | *\n",
      "Epoch 00010 | Time(s) 0.2180 | Loss(train) 1.1589 | Acc(train) 0.6929 | Loss(val) 1.1647 | Acc(val) 0.7000 | *\n",
      "Epoch 00011 | Time(s) 0.2626 | Loss(train) 1.0464 | Acc(train) 0.7357 | Loss(val) 1.0648 | Acc(val) 0.7100 | *\n",
      "Epoch 00012 | Time(s) 0.2431 | Loss(train) 0.9464 | Acc(train) 0.7214 | Loss(val) 0.9757 | Acc(val) 0.7440 | *\n",
      "Epoch 00013 | Time(s) 0.2235 | Loss(train) 0.8613 | Acc(train) 0.7571 | Loss(val) 0.8989 | Acc(val) 0.7540 | *\n",
      "Epoch 00014 | Time(s) 0.2242 | Loss(train) 0.7798 | Acc(train) 0.8143 | Loss(val) 0.8349 | Acc(val) 0.7660 | *\n",
      "Epoch 00015 | Time(s) 0.2249 | Loss(train) 0.7172 | Acc(train) 0.8357 | Loss(val) 0.7836 | Acc(val) 0.7640 | \n",
      "Epoch 00016 | Time(s) 0.2222 | Loss(train) 0.6676 | Acc(train) 0.8286 | Loss(val) 0.7478 | Acc(val) 0.7620 | \n",
      "Epoch 00017 | Time(s) 0.2076 | Loss(train) 0.6345 | Acc(train) 0.8500 | Loss(val) 0.7216 | Acc(val) 0.7720 | *\n",
      "Epoch 00018 | Time(s) 0.2070 | Loss(train) 0.5878 | Acc(train) 0.8571 | Loss(val) 0.7055 | Acc(val) 0.7700 | \n",
      "Epoch 00019 | Time(s) 0.2092 | Loss(train) 0.5630 | Acc(train) 0.8571 | Loss(val) 0.6982 | Acc(val) 0.7680 | \n",
      "Epoch 00020 | Time(s) 0.2298 | Loss(train) 0.5344 | Acc(train) 0.8286 | Loss(val) 0.7038 | Acc(val) 0.7620 | \n",
      "Epoch 00021 | Time(s) 0.2310 | Loss(train) 0.5061 | Acc(train) 0.8429 | Loss(val) 0.7071 | Acc(val) 0.7600 | \n",
      "Epoch 00022 | Time(s) 0.2309 | Loss(train) 0.4907 | Acc(train) 0.8429 | Loss(val) 0.7043 | Acc(val) 0.7600 | \n",
      "Epoch 00023 | Time(s) 0.2338 | Loss(train) 0.4674 | Acc(train) 0.8571 | Loss(val) 0.6947 | Acc(val) 0.7680 | \n",
      "Epoch 00024 | Time(s) 0.2193 | Loss(train) 0.4528 | Acc(train) 0.8714 | Loss(val) 0.6899 | Acc(val) 0.7700 | \n",
      "Epoch 00025 | Time(s) 0.2104 | Loss(train) 0.4306 | Acc(train) 0.8714 | Loss(val) 0.6909 | Acc(val) 0.7700 | \n",
      "Epoch 00026 | Time(s) 0.2098 | Loss(train) 0.4196 | Acc(train) 0.8929 | Loss(val) 0.6975 | Acc(val) 0.7680 | \n",
      "Epoch 00027 | Time(s) 0.2100 | Loss(train) 0.3981 | Acc(train) 0.8714 | Loss(val) 0.7090 | Acc(val) 0.7680 | \n",
      "Epoch 00028 | Time(s) 0.2106 | Loss(train) 0.3892 | Acc(train) 0.8857 | Loss(val) 0.7168 | Acc(val) 0.7680 | \n",
      "Epoch 00029 | Time(s) 0.2191 | Loss(train) 0.3748 | Acc(train) 0.8786 | Loss(val) 0.7159 | Acc(val) 0.7640 | \n",
      "Epoch 00030 | Time(s) 0.2093 | Loss(train) 0.3546 | Acc(train) 0.8929 | Loss(val) 0.7130 | Acc(val) 0.7700 | \n",
      "Epoch 00031 | Time(s) 0.2127 | Loss(train) 0.3572 | Acc(train) 0.8929 | Loss(val) 0.7070 | Acc(val) 0.7760 | *\n",
      "Epoch 00032 | Time(s) 0.2093 | Loss(train) 0.3465 | Acc(train) 0.8857 | Loss(val) 0.7019 | Acc(val) 0.7740 | \n",
      "Epoch 00033 | Time(s) 0.2089 | Loss(train) 0.3374 | Acc(train) 0.9000 | Loss(val) 0.7007 | Acc(val) 0.7780 | *\n",
      "Epoch 00034 | Time(s) 0.2100 | Loss(train) 0.3249 | Acc(train) 0.8929 | Loss(val) 0.7150 | Acc(val) 0.7800 | *\n",
      "Epoch 00035 | Time(s) 0.2078 | Loss(train) 0.3189 | Acc(train) 0.8929 | Loss(val) 0.7314 | Acc(val) 0.7700 | \n",
      "Epoch 00036 | Time(s) 0.2101 | Loss(train) 0.3123 | Acc(train) 0.9071 | Loss(val) 0.7387 | Acc(val) 0.7700 | \n",
      "Epoch 00037 | Time(s) 0.2093 | Loss(train) 0.3096 | Acc(train) 0.9143 | Loss(val) 0.7360 | Acc(val) 0.7720 | \n",
      "Epoch 00038 | Time(s) 0.2082 | Loss(train) 0.3010 | Acc(train) 0.8929 | Loss(val) 0.7318 | Acc(val) 0.7760 | \n",
      "Epoch 00039 | Time(s) 0.2084 | Loss(train) 0.2956 | Acc(train) 0.9071 | Loss(val) 0.7264 | Acc(val) 0.7760 | \n",
      "Epoch 00040 | Time(s) 0.2087 | Loss(train) 0.2921 | Acc(train) 0.9000 | Loss(val) 0.7298 | Acc(val) 0.7780 | \n",
      "Epoch 00041 | Time(s) 0.2138 | Loss(train) 0.2876 | Acc(train) 0.9071 | Loss(val) 0.7349 | Acc(val) 0.7800 | \n",
      "Epoch 00042 | Time(s) 0.2561 | Loss(train) 0.2821 | Acc(train) 0.9214 | Loss(val) 0.7375 | Acc(val) 0.7800 | \n",
      "Epoch 00043 | Time(s) 0.2392 | Loss(train) 0.2738 | Acc(train) 0.9214 | Loss(val) 0.7315 | Acc(val) 0.7800 | \n",
      "Epoch 00044 | Time(s) 0.2200 | Loss(train) 0.2711 | Acc(train) 0.9214 | Loss(val) 0.7321 | Acc(val) 0.7840 | *\n",
      "Epoch 00045 | Time(s) 0.2165 | Loss(train) 0.2705 | Acc(train) 0.9143 | Loss(val) 0.7460 | Acc(val) 0.7740 | \n",
      "Epoch 00046 | Time(s) 0.2082 | Loss(train) 0.2618 | Acc(train) 0.9143 | Loss(val) 0.7641 | Acc(val) 0.7720 | \n",
      "Epoch 00047 | Time(s) 0.2052 | Loss(train) 0.2617 | Acc(train) 0.9286 | Loss(val) 0.7751 | Acc(val) 0.7680 | \n",
      "Epoch 00048 | Time(s) 0.2187 | Loss(train) 0.2561 | Acc(train) 0.9143 | Loss(val) 0.7753 | Acc(val) 0.7700 | \n",
      "Epoch 00049 | Time(s) 0.2272 | Loss(train) 0.2455 | Acc(train) 0.9214 | Loss(val) 0.7665 | Acc(val) 0.7720 | \n",
      "Epoch 00050 | Time(s) 0.2255 | Loss(train) 0.2508 | Acc(train) 0.9214 | Loss(val) 0.7524 | Acc(val) 0.7820 | \n",
      "Epoch 00051 | Time(s) 0.2078 | Loss(train) 0.2464 | Acc(train) 0.9214 | Loss(val) 0.7434 | Acc(val) 0.7780 | \n",
      "Epoch 00052 | Time(s) 0.2104 | Loss(train) 0.2339 | Acc(train) 0.9214 | Loss(val) 0.7510 | Acc(val) 0.7800 | \n",
      "Epoch 00053 | Time(s) 0.2117 | Loss(train) 0.2293 | Acc(train) 0.9214 | Loss(val) 0.7651 | Acc(val) 0.7780 | \n",
      "Epoch 00054 | Time(s) 0.2594 | Loss(train) 0.2336 | Acc(train) 0.9071 | Loss(val) 0.7799 | Acc(val) 0.7760 | \n",
      "Epoch 00055 | Time(s) 0.2435 | Loss(train) 0.2275 | Acc(train) 0.9214 | Loss(val) 0.7896 | Acc(val) 0.7700 | \n",
      "Epoch 00056 | Time(s) 0.2211 | Loss(train) 0.2193 | Acc(train) 0.9357 | Loss(val) 0.7987 | Acc(val) 0.7660 | \n",
      "Epoch 00057 | Time(s) 0.2119 | Loss(train) 0.2233 | Acc(train) 0.9214 | Loss(val) 0.7945 | Acc(val) 0.7680 | \n",
      "Epoch 00058 | Time(s) 0.2064 | Loss(train) 0.2170 | Acc(train) 0.9214 | Loss(val) 0.7810 | Acc(val) 0.7780 | \n",
      "Epoch 00059 | Time(s) 0.2079 | Loss(train) 0.2234 | Acc(train) 0.9143 | Loss(val) 0.7723 | Acc(val) 0.7840 | \n",
      "Epoch 00060 | Time(s) 0.2085 | Loss(train) 0.2117 | Acc(train) 0.9286 | Loss(val) 0.7791 | Acc(val) 0.7800 | \n",
      "Epoch 00061 | Time(s) 0.2079 | Loss(train) 0.2122 | Acc(train) 0.9214 | Loss(val) 0.7971 | Acc(val) 0.7760 | \n",
      "Epoch 00062 | Time(s) 0.2060 | Loss(train) 0.2044 | Acc(train) 0.9357 | Loss(val) 0.8178 | Acc(val) 0.7700 | \n",
      "Epoch 00063 | Time(s) 0.2066 | Loss(train) 0.2067 | Acc(train) 0.9357 | Loss(val) 0.8312 | Acc(val) 0.7680 | \n",
      "Epoch 00064 | Time(s) 0.2079 | Loss(train) 0.1988 | Acc(train) 0.9429 | Loss(val) 0.8402 | Acc(val) 0.7720 | \n",
      "Epoch 00065 | Time(s) 0.2125 | Loss(train) 0.1998 | Acc(train) 0.9357 | Loss(val) 0.8379 | Acc(val) 0.7720 | \n",
      "Epoch 00066 | Time(s) 0.2101 | Loss(train) 0.1916 | Acc(train) 0.9429 | Loss(val) 0.8322 | Acc(val) 0.7740 | \n",
      "Epoch 00067 | Time(s) 0.2116 | Loss(train) 0.1812 | Acc(train) 0.9429 | Loss(val) 0.8351 | Acc(val) 0.7780 | \n",
      "Epoch 00068 | Time(s) 0.2144 | Loss(train) 0.1817 | Acc(train) 0.9500 | Loss(val) 0.8425 | Acc(val) 0.7780 | \n",
      "Epoch 00069 | Time(s) 0.2100 | Loss(train) 0.1768 | Acc(train) 0.9429 | Loss(val) 0.8526 | Acc(val) 0.7760 | \n",
      "Epoch 00070 | Time(s) 0.2101 | Loss(train) 0.1665 | Acc(train) 0.9571 | Loss(val) 0.8571 | Acc(val) 0.7680 | \n",
      "Epoch 00071 | Time(s) 0.2110 | Loss(train) 0.1654 | Acc(train) 0.9571 | Loss(val) 0.8603 | Acc(val) 0.7720 | \n",
      "Epoch 00072 | Time(s) 0.2090 | Loss(train) 0.1618 | Acc(train) 0.9571 | Loss(val) 0.8579 | Acc(val) 0.7760 | \n",
      "Epoch 00073 | Time(s) 0.2093 | Loss(train) 0.1563 | Acc(train) 0.9571 | Loss(val) 0.8561 | Acc(val) 0.7700 | \n",
      "Epoch 00074 | Time(s) 0.2096 | Loss(train) 0.1567 | Acc(train) 0.9571 | Loss(val) 0.8594 | Acc(val) 0.7740 | \n",
      "Epoch 00075 | Time(s) 0.2101 | Loss(train) 0.1465 | Acc(train) 0.9643 | Loss(val) 0.8653 | Acc(val) 0.7660 | \n",
      "Epoch 00076 | Time(s) 0.2116 | Loss(train) 0.1435 | Acc(train) 0.9500 | Loss(val) 0.8729 | Acc(val) 0.7640 | \n",
      "Epoch 00077 | Time(s) 0.2102 | Loss(train) 0.1386 | Acc(train) 0.9643 | Loss(val) 0.8804 | Acc(val) 0.7540 | \n",
      "Epoch 00078 | Time(s) 0.2099 | Loss(train) 0.1379 | Acc(train) 0.9714 | Loss(val) 0.8966 | Acc(val) 0.7460 | \n",
      "Epoch 00079 | Time(s) 0.2097 | Loss(train) 0.1352 | Acc(train) 0.9571 | Loss(val) 0.9094 | Acc(val) 0.7420 | \n",
      "Epoch 00080 | Time(s) 0.2113 | Loss(train) 0.1348 | Acc(train) 0.9714 | Loss(val) 0.9181 | Acc(val) 0.7460 | \n",
      "Epoch 00081 | Time(s) 0.2099 | Loss(train) 0.1241 | Acc(train) 0.9643 | Loss(val) 0.9166 | Acc(val) 0.7480 | \n",
      "Epoch 00082 | Time(s) 0.2098 | Loss(train) 0.1244 | Acc(train) 0.9786 | Loss(val) 0.9176 | Acc(val) 0.7500 | \n",
      "Epoch 00083 | Time(s) 0.2085 | Loss(train) 0.1198 | Acc(train) 0.9857 | Loss(val) 0.9207 | Acc(val) 0.7460 | \n",
      "Epoch 00084 | Time(s) 0.2098 | Loss(train) 0.1162 | Acc(train) 0.9786 | Loss(val) 0.9195 | Acc(val) 0.7520 | \n",
      "Epoch 00085 | Time(s) 0.2105 | Loss(train) 0.1135 | Acc(train) 0.9786 | Loss(val) 0.9181 | Acc(val) 0.7600 | \n",
      "Epoch 00086 | Time(s) 0.2110 | Loss(train) 0.1091 | Acc(train) 0.9786 | Loss(val) 0.9245 | Acc(val) 0.7540 | \n",
      "Epoch 00087 | Time(s) 0.2111 | Loss(train) 0.1040 | Acc(train) 0.9786 | Loss(val) 0.9428 | Acc(val) 0.7600 | \n",
      "Epoch 00088 | Time(s) 0.2102 | Loss(train) 0.1011 | Acc(train) 0.9857 | Loss(val) 0.9594 | Acc(val) 0.7560 | \n",
      "Epoch 00089 | Time(s) 0.2104 | Loss(train) 0.1044 | Acc(train) 0.9857 | Loss(val) 0.9729 | Acc(val) 0.7600 | \n",
      "Epoch 00090 | Time(s) 0.2116 | Loss(train) 0.1022 | Acc(train) 0.9786 | Loss(val) 0.9721 | Acc(val) 0.7540 | \n",
      "Epoch 00091 | Time(s) 0.2111 | Loss(train) 0.0986 | Acc(train) 0.9786 | Loss(val) 0.9672 | Acc(val) 0.7580 | \n",
      "Epoch 00092 | Time(s) 0.2094 | Loss(train) 0.0963 | Acc(train) 0.9857 | Loss(val) 0.9582 | Acc(val) 0.7600 | \n",
      "Epoch 00093 | Time(s) 0.2044 | Loss(train) 0.0963 | Acc(train) 0.9786 | Loss(val) 0.9610 | Acc(val) 0.7600 | \n",
      "Epoch 00094 | Time(s) 0.2030 | Loss(train) 0.0925 | Acc(train) 0.9857 | Loss(val) 0.9718 | Acc(val) 0.7600 | \n",
      "Epoch 00095 | Time(s) 0.2024 | Loss(train) 0.0893 | Acc(train) 0.9857 | Loss(val) 0.9901 | Acc(val) 0.7620 | \n",
      "Epoch 00096 | Time(s) 0.2040 | Loss(train) 0.0900 | Acc(train) 0.9857 | Loss(val) 1.0082 | Acc(val) 0.7600 | \n",
      "Epoch 00097 | Time(s) 0.2031 | Loss(train) 0.0835 | Acc(train) 0.9857 | Loss(val) 1.0333 | Acc(val) 0.7540 | \n",
      "Epoch 00098 | Time(s) 0.2025 | Loss(train) 0.0895 | Acc(train) 0.9929 | Loss(val) 1.0503 | Acc(val) 0.7500 | \n",
      "Epoch 00099 | Time(s) 0.2023 | Loss(train) 0.0875 | Acc(train) 0.9857 | Loss(val) 1.0528 | Acc(val) 0.7500 | \n",
      "Epoch 00100 | Time(s) 0.2029 | Loss(train) 0.0854 | Acc(train) 0.9929 | Loss(val) 1.0484 | Acc(val) 0.7540 | \n",
      "Optimization Finished!\n",
      "Time(s): 9.6453\n",
      "Loss(test) 0.5642 | Acc(test) 0.8290\n",
      "All runs:\n",
      "Highest Train: 91.43 ± 2.58\n",
      "Highest Valid: 78.80 ± 0.40\n",
      "   Final Test: 82.97 ± 0.31\n"
     ]
    },
    {
     "data": {
      "text/plain": "(82.96666666666665, 0.3055050463303937)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from opengsl.module.solver import GSLSolver\n",
    "from opengsl import ExpManager\n",
    "import argparse\n",
    "\n",
    "class MyGSL(GSLSolver):\n",
    "    def set_method(self):\n",
    "        encoder = GCNDiagEncoder(2, dataset.dim_feats)\n",
    "        metric = Cosine()\n",
    "        postprocess = [KNN(150)]\n",
    "        fuse = Interpolate(1, 1)\n",
    "        # build the graphlearner\n",
    "        self.graphlearner = GraphLearner(encoder=encoder, metric=metric, postprocess=postprocess, fuse=fuse).to(device)\n",
    "        # define gnn model\n",
    "        self.model = GCNEncoder(dataset.dim_feats, nhid=64, nclass=dataset.n_classes, n_layers=2, dropout=0.5).to(device)\n",
    "        self.optim = torch.optim.Adam([{'params': self.model.parameters()}, {'params': self.graphlearner.parameters()}], lr=self.conf.training['lr'], weight_decay=self.conf.training['weight_decay'])\n",
    "\n",
    "conf = {'model': {'n_hidden': 64, 'n_layer': 2},\n",
    "    'training': {'lr': 1e-2,\n",
    "    'weight_decay': 5e-4,\n",
    "    'n_epochs': 100,\n",
    "    'patience': None,\n",
    "    'criterion': 'metric'},\n",
    "    'dataset': {'feat_norm': False, 'sparse': True},\n",
    "    'analysis': {'flag': False, 'save_graph': False}}\n",
    "mygsl = MyGSL(argparse.Namespace(**conf), dataset)\n",
    "exp = ExpManager(solver=mygsl)\n",
    "exp.run(n_runs=3, debug=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now you have learned how to build a simple GSL using our provided components in OpenGSL. Try other components and datasets freely~"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
